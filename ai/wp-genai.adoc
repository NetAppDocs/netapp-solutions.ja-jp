---
sidebar: sidebar 
permalink: ai/wp-genai.html 
keywords: NetApp AI, AI, Artificial Intelligence, Generative AI 
summary: NetApp®のAI機能により、生成型AIモデルのトレーニング、再トレーニング、微調整、推論、監視のために、AIパイプライン全体でのシームレスなデータ管理とデータ移動が可能になります。 
---
= AIとNetAppの価値を生成
:allow-uri-read: 


[role="lead"]
生成型人工知能（AI）の需要は、さまざまな業界の混乱を引き起こし、ビジネスの創造性と製品のイノベーションを強化しています。



== 概要

多くの組織が、ジェネレーティブAIを使用して、新しい製品機能の構築、エンジニアリングの生産性の向上、より優れた結果と消費者体験を提供するAIベースのアプリケーションのプロトタイプ作成を行っています。Generative Pre-Trained Transformers（GPT）などのジェネレーティブAIは、ニューラルネットワークを使用して、テキスト、オーディオ、ビデオなど多様な新しいコンテンツを作成します。大規模な言語モデル（LLM）に関連する膨大な規模のデータセットを考えると、オンプレミス、ハイブリッド、マルチクラウドの導入オプションにある魅力的なデータストレージ機能を活用し、データモビリティに関連するリスクを軽減する堅牢なAIインフラを設計することが重要です。 企業がAIソリューションを設計する前に、データ保護とガバナンスを強化しましょう。このホワイトペーパーでは、これらの考慮事項と、生成型AIモデルのトレーニング、再トレーニング、微調整、推論のためのAIデータパイプライン全体でのシームレスなデータ管理とデータ移動を可能にする、対応するNetApp®AIの機能について説明します。



== エグゼクティブサマリー

最近では、2022年11月にGPT-3のスピンオフであるChatGPTがリリースされた後、テキスト、コード、画像、さらには治療用タンパク質をユーザーのプロンプトに応答して生成するために使用される新しいAIツールが大きな名声を得ています。これは、ユーザーが自然言語を使用して要求を行うことができることを示しています。AIは、ユーザーの要求を反映したニュース記事や製品説明などのテキストを解釈して生成したり、既存のデータでトレーニングされたアルゴリズムを使用してコード、音楽、スピーチ、視覚効果、3Dアセットを生成したりします。その結果、AIシステムの設計には、「安定した拡散」、「幻覚」、「迅速なエンジニアリング」、「価値の整合」などのフレーズが急速に登場しています。これらの自己管理型または半管理型機械学習（ML）モデルは、クラウドサービスプロバイダや他のAIベンダーを通じて、事前トレーニング済みの基礎モデル（FM）として広く利用されるようになり、さまざまな業界のさまざまな事業所で、下流の自然言語処理（NLP）タスクに採用されています。マッキンゼーのような調査アナリスト企業は、「ジェネレーティブAIが生産性に与える影響は、世界経済に数兆ドルの価値をもたらす可能性がある」と主張しています。企業はAIを人間の思考パートナーとして再定義しつつあり、FMSは、企業や機関がジェネレーティブAIでできることを同時に拡大していますが、膨大な量のデータを管理する機会は今後も増え続けるでしょう。このドキュメントでは、オンプレミス環境とハイブリッド環境またはマルチクラウド環境の両方で、NetAppのお客様に価値をもたらすNetApp機能に関連した、生成型AIの概要と設計の概念について説明します。

*では、お客様がAI環境でNetAppを使用することには、どのようなメリットがありますか？*NetAppは、データとクラウドの急速な拡大、マルチクラウド管理、AIなどの次世代テクノロジの採用によって生じる複雑さに対応するのに役立ちます。NetAppは、さまざまな機能をインテリジェントなデータ管理ソフトウェアとストレージインフラに統合し、AIワークロード向けに最適化されたハイパフォーマンスとのバランスを良好に保ちました。LLMのような生成型AIソリューションでは、インテリジェンスを強化するために、ストレージからソースデータセットを何度も読み取り、処理してメモリに格納する必要があります。NetAppは、エッジからコア、クラウドまでのエコシステム全体で、データモビリティ、データガバナンス、データセキュリティのテクノロジ分野で業界をリードしており、大規模なAIソリューションを構築する大企業のお客様にサービスを提供しています。NetAppには強力なパートナーネットワークがあり、最高データ責任者、AIエンジニア、エンタープライズアーキテクト、データサイエンティストが自由に流れるデータパイプラインを設計し、データの前処理、データ保護、 AIモデルのトレーニングと推論に関する戦略的データ管理責任を負い、AI / MLライフサイクルのパフォーマンスと拡張性を最適化します。NetAppのデータテクノロジと機能には、ディープラーニングデータパイプライン向けのNetApp®ONTAP AI®、ストレージエンドポイント間でシームレスかつ効率的にデータを転送するNetApp®SnapMirror®などがあります。 また、リアルタイムレンダリングのためのNetApp®FlexCache®データフローがバッチからリアルタイムに移行し、データエンジニアリングが迅速に行われる場合、リアルタイムジェネレーティブAIモデルの導入に価値をもたらします。あらゆるタイプの企業が新しいAIツールを導入する際、エッジからデータセンター、クラウドまで、拡張性と責任性に優れた説明可能なAIソリューションが求められるデータの課題に直面します。ハイブリッドクラウドとマルチクラウドにおけるデータ管理のオーソリティであるNetAppは、パートナーと共同ソリューションのネットワークの構築に取り組んでいます。このネットワークは、生成AIモデルのトレーニング（プレトレーニング）、微調整、コンテキストベースの推論、LLMのモデル崩壊監視のためのデータパイプラインとデータレイクの構築のあらゆる側面を支援します。



== ジェネレーティブAIとは

ジェネレーティブAIは、コンテンツの作成方法、新しいデザインコンセプトの生成方法、新しい構成の探索方法を変えています。ここでは、Generative Adversarial Network（GAN）、Variational Autoencoders（VAE）、Generative Pre-Trained Transformers（GPT）などのニューラルネットワークフレームワークについて説明します。これらのフレームワークは、テキスト、コード、画像、オーディオ、ビデオ、 合成データもありますOpenAIのChat-GPT、GoogleのBard、Hugging Face’s Bloom、Meta’s llamaなどのトランスベースモデルは、大規模な言語モデルの多くの進歩を支える基盤技術として登場しています。同様に、OpenAIのdall-E、MetaのCM3leon、GoogleのImagenは、テキストと画像のセマンティクスをリンクするデータセット拡張とテキストと画像の合成を使用して、新しい複雑な画像をゼロから作成したり、既存の画像を編集して高品質のコンテキスト認識画像を生成する、かつてないほどのフォトリアリズムを提供するテキスト間拡散モデルの例です。デジタルアーティストは、静的な2D画像を没入型の3Dシーンに変換するために、Nerf（Neural Radiance Field）などのレンダリング技術をジェネレーティブAIと組み合わせて適用し始めています。一般に、LLMは、(1)モデルのサイズ(通常は数十億個のパラメータ)、(2)トレーニングデータセットのサイズ、(3)トレーニングコスト、(4)トレーニング後のモデルパフォーマンスの4つのパラメータによって大まかに特徴付けられます。LLMは、主に3つの変圧器アーキテクチャに分類されます。（i）エンコーダのみの機種例：BERT（Google、2018）、（ii）エンコーダ-デコーダ例：BART（Meta、2020）、（iii）デコーダのみのモデル。例：llama（Meta、2023）、palm-E（Google、2023）。ビジネス要件に応じて、モデルパラメータの数（N）とトレーニングデータセット内のトークンの数（D）を選択するアーキテクチャに関係なく、一般にトレーニングのベースラインコスト（プレトレーニング）またはLLMの微調整を決定します。



=== エンタープライズユースケースと下流工程の自然言語処理タスク

さまざまな業界の企業が、AIが既存のデータから新しい形の価値を引き出し、創出する可能性をますます明らかにしています。これは、事業運営、営業、マーケティング、法務サービスのためです。グローバルジェネレーティブAIのユースケースと投資に関するIDC（International Data Corporation）のマーケットインテリジェンスによると、ソフトウェア開発と製品設計におけるナレッジ管理が最も影響を受け、続いて開発者向けのマーケティングとコード生成のストーリーライン作成が行われます。医療の分野では、臨床研究機関が医療の新しい分野を開拓しています。ProteinBERTのような事前トレーニングされたモデルには、Gene Ontology（GO）アノテーションが組み込まれており、医薬品のタンパク質構造を迅速に設計できます。これは、創薬、バイオインフォマティクス、分子生物学における重要なマイルストーンとなります。バイオテクノロジー企業は、人工知能によって発見された生成医療のためのヒト試験を開始しました。これは、肺線維症(IPF)のような肺組織の不可逆的な瘢痕化を引き起こす肺疾患の治療を目的としたものです。

図1：ジェネレーティブAIの推進要因となっているユースケース

image:gen-ai-image1.png["図1：ジェネレーティブAIの推進要因となっているユースケース"]

ジェネレーティブAIによる自動化の採用の増加は、多くの職種の業務活動の需要と供給にも変化をもたらしています。マッキンゼーによると、米国の労働市場（下の図）は急速な変化を遂げており、この変化はAIの影響を考慮した場合にのみ継続する可能性があります。

出典：McKinsey & Company

image:gen-ai-image3.png["図2：出典：McKinsey  amp; Company"]



=== ジェネレーティブAIにおけるストレージの役割

LLMは主にディープラーニング、GPU、コンピューティングに依存しています。ただし、GPUバッファがいっぱいになると、データをストレージにすばやく書き込む必要があります。一部のAIモデルはメモリで実行できるほど小型ではありますが、LLMでは大規模なデータセットへの高速アクセスを実現するために、高いIOPSと高スループットのストレージが必要です。特に、数十億のトークンや数百万の画像が含まれる場合はなおさらです。LLMの一般的なGPUメモリ要件では、10億個のパラメータを持つモデルをトレーニングするために必要なメモリは、32ビットの完全精度で最大80GBになる可能性があります。その場合、Meta’s llama 2は、70億から700億のパラメータの規模のLLMファミリーで、70x80、約5600GB、または5.6TBのGPU RAMが必要になる場合があります。さらに、必要なメモリの量は、生成するトークンの最大数に直接比例します。たとえば、最大512トークン(約380ワード)の出力を生成する場合は、が必要ですlink:https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model["512 MB"]。重要ではないように思えるかもしれませんが、より大きなバッチを実行したい場合は、合計が始まります。そのため、組織がメモリ内でLLMのトレーニングや微調整を行うには非常にコストがかかり、ストレージは生成AIの基盤となります。



=== LLMへの3つの主なアプローチ

ほとんどの企業では、現在の傾向に基づいて、LLMの導入アプローチを3つの基本シナリオにまとめることができます。最近の記事で説明されているように、link:https://hbr.org/2023/07/how-to-train-generative-ai-using-your-companys-data["「Harvard Business Review」"](1)ゼロからのLLMのトレーニング(事前トレーニング)–コストがかかり、専門的なAI/MLスキルが必要です。(2)エンタープライズデータを使用した基盤モデルの微調整–複雑で実行可能です。(3)検索拡張生成(RAG)を使用して、企業データを含むドキュメントリポジトリ、API、ベクターデータベースを照会します。これらのそれぞれには、さまざまなタイプの問題を解決するために使用される、実装における労力、反復速度、コスト効率、モデルの精度の間にトレードオフがあります(下の図)。

図3：問題の種類

image:gen-ai-image4.png["図3：問題の種類"]



=== 基盤モデル

基礎モデル(FM)は、ベースモデルとも呼ばれ、ラベル付けされていない膨大な量のデータでトレーニングされ、大規模な自己管理を使用して、一般的に下流のNLPタスクの広い範囲に適応された大規模なAIモデル(LLM)です。トレーニングデータは人間によってラベル付けされていないため、モデルは明示的にエンコードされるのではなく出現する。これは、モデルが明示的にプログラムされていなくても、ストーリーや独自の物語を生成できることを意味します。したがってFMの重要な特徴は均質化であり、同じ方法が多くの領域で使われていることを意味する。しかし、パーソナライゼーションと微調整の技術により、最近登場した製品に統合されたFMSは、テキスト、テキストから画像、テキストからコードの生成だけでなく、ドメイン固有のタスクやデバッグコードの説明にも適しています。例えば、OpenAIのCodexやMetaのCode LlamaのようなFMSは、プログラミングタスクの自然言語記述に基づいて複数のプログラミング言語でコードを生成することができる。これらのモデルは、Python、C#、JavaScript、Perl、Ruby、 およびSQLを使用します。ユーザーの意図を理解し、ソフトウェア開発、コードの最適化、プログラミングタスクの自動化に役立つ目的のタスクを実行する特定のコードを生成します。



=== 微調整、ドメイン特異性、再トレーニング

データ前処理とデータ前処理に続くLLM導入では、大規模で多様なデータセットでトレーニングされた事前トレーニングモデルを選択することが一般的です。微調整の文脈では、これは700億個のパラメータと2兆個のトークンでトレーニングされたようなオープンソースの大規模言語モデルになる可能性がありますlink:https://ai.meta.com/llama/["Meta's Llama 2"]。事前トレーニング済みモデルを選択したら、次のステップでは、ドメイン固有のデータに基づいてモデルを微調整します。これには、モデルのパラメータを調整し、特定のドメインやタスクに適応するように新しいデータをトレーニングすることが含まれます。たとえば、BloombergGPTは、金融業界にサービスを提供する幅広い金融データのトレーニングを受けた独自のLLMです。特定のタスクのために設計され訓練されたドメイン固有のモデルは、通常、その範囲内でより高い精度とパフォーマンスを発揮しますが、他のタスクやドメイン間での転送性は低くなります。ビジネス環境やデータが一定期間にわたって変化すると、FMの予測精度は、テスト中のパフォーマンスと比較して低下し始める可能性があります。これは、モデルの再トレーニングや微調整が重要になるときです。従来のAI / MLでのモデルの再トレーニングとは、導入したMLモデルを新しいデータで更新することを指します。通常、2種類のドリフトを排除するために実行されます。(1)概念ドリフト–入力変数とターゲット変数のリンクが時間の経過とともに変化すると、変化を予測したいものの概要が発生するため、モデルは不正確な予測を生成する可能性があります。(2)データドリフト–入力データの特性が変化し、時間の経過とともに顧客の習慣や行動が変化し、モデルがそのような変化に対応できない場合に発生します。同様の方法で、環境FMS/LLMの再トレーニングを行いますが、コストが高くなる可能性があります(数百万ドル)。したがって、ほとんどの組織が検討することはできません。現在も活発な研究が行われており、LLMOpsの分野で発展している。そのため、再トレーニングの代わりに、微調整されたFMSでモデルの崩壊が発生した場合、企業は新しいデータセットで再び微調整(はるかに安価)を選択することができます。コストの観点から、以下はAzure-OpenAI Servicesのモデル価格表の例です。タスクカテゴリごとに、特定のデータセットのモデルを微調整して評価できます。

出典：Microsoft Azure

image:gen-ai-image5.png["出典：Microsoft Azure"]



=== 迅速なエンジニアリングと推論

プロンプトエンジニアリングとは、モデルの重みを更新せずに必要なタスクを実行するためにLLMと通信する効果的な方法を指します。AIモデルのトレーニングと微調整が自然言語処理アプリケーションにとって重要であるのと同じように、推論も同様に重要であり、トレーニング済みモデルがユーザプロンプトに応答します。一般に、推論のシステム要件は、最適な応答を生成するために数十億個の保存モデルパラメータを適用できる必要があるため、LLMからGPUにデータを供給するAIストレージシステムの読み取りパフォーマンスにはるかに依存します。



=== LLMOps、モデルモニタリング、およびベクトルストア

従来の機械学習運用（MLOps）と同様に、Large Language Model Operations（LLMOps）でも、データサイエンティストやDevOpsエンジニアと、本番環境でLLMを管理するためのツールやベストプラクティスを連携させる必要があります。ただし、LLMのワークフローと技術スタックは、いくつかの点で異なる場合があります。たとえば、LangChain stringなどのフレームワークを使用して構築されたLLMパイプラインは、ベクトルストアやベクトルデータベースなどの外部埋め込みエンドポイントへの複数のLLM API呼び出しを組み合わせて構築されます。（ベクターデータベースのように）ダウンストリームコネクタに埋め込みエンドポイントとベクトルストアを使用することは、データの格納方法とアクセス方法の重要な発展を表しています。ゼロから開発された従来のMLモデルとは異なり、LLMは、より特定の領域でパフォーマンスを向上させるために新しいデータで微調整されたFMSから始まるため、転送学習に依存することがよくあります。したがって、LLMOPは、リスク管理とモデル崩壊モニタリングの機能を提供することが非常に重要です。



=== ジェネレーティブAIの時代におけるリスクと倫理

「ChatGPT–It's slick but still spews nonsense.」–MIT Tech Review.ガベージイン-ガベージアウトは、コンピューティングにおいて常に困難な課題でした。生成型AIとの唯一の違いは、ごみの信頼性が高く、結果が不正確になることです。LLMは、構築している物語に合うように事実を発明する傾向があります。そのため、生成型AIを同等のAIでコストを削減する絶好の機会と見なしている企業は、システムを正直で倫理的に保つために、ディープフェイクを効率的に検出し、バイアスを減らし、リスクを軽減する必要があります。エンドツーエンドの暗号化とAIガードレールにより、データモビリティ、データ品質、データガバナンス、データ保護をサポートする堅牢なAIインフラを備えた自由に流れるデータパイプラインは、責任ある説明可能な生成AIモデルの設計において卓越しています。



== お客様のシナリオとNetApp

図3：機械学習/大規模言語モデルのワークフロー

image:gen-ai-image6.png["図3：機械学習/大規模言語モデルのワークフロー"]

*トレーニングや微調整を行っていますか？*（a）LLMモデルをゼロからトレーニングするか、事前にトレーニングされたFMを微調整するか、RAGを使用して基礎モデル以外のドキュメントリポジトリからデータを取得してプロンプトを強化するか、（b）オープンソースLLM（Llama 2など）または独自のFMS（ChatGPT、Bard、AWS Bedrockなど）を活用するかは、組織にとって戦略的な決定です。各アプローチには、コスト効率、データの重力、運用、モデルの精度、LLMの管理のトレードオフがあります。

企業としてのNetAppは、社内のワークカルチャーや、製品設計やエンジニアリングの取り組みにAIを取り入れています。たとえば、ネットアップの自律型ランサムウェア対策は、AIと機械学習を使用して構築されています。ファイルシステムの異常を早期に検出し、運用に影響が及ぶ前に脅威を特定するのに役立ちます。次に、NetAppは、販売や在庫予測、チャットボットなどのビジネスオペレーションに予測AIを使用して、コールセンター製品サポートサービス、技術仕様、保証、サービスマニュアルなどの顧客を支援します。3つ目は、NetAppが、需要予測、医療画像処理、センチメント分析などの予測AIソリューションを構築するお客様にサービスを提供する製品とソリューションを通じて、AIデータパイプラインとML / LLMワークフローでお客様に価値を提供することです。 また、NetApp®ONTAP AI®、NetApp®SnapMirror®、NetApp®FlexCache®などのNetApp製品と機能を使用して、製造部門での産業画像の異常検出や、銀行や金融サービスでのマネーロンダリング防止や不正検出に対応するGANなどの生成AIソリューションも提供します。



== NetAppの機能

チャットボット、コード生成、画像生成、ゲノムモデル表現などの生成AIアプリケーションでのデータの移動と管理は、エッジ、プライベートデータセンター、ハイブリッドマルチクラウドエコシステム全体にわたって可能です。例えば、ChatGPTのような事前訓練されたモデルのAPIを介して公開されたエンドユーザーアプリから航空券をビジネスクラスにアップグレードするのを支援するリアルタイムAIボットは、乗客情報がインターネット上で公開されていないため、単独でそのタスクを達成することはできません。APIは、ハイブリッドまたはマルチクラウドエコシステムに存在する可能性のある航空会社からの乗客の個人情報とチケット情報にアクセスする必要があります。同様のシナリオは、LLMを使用して1対多のバイオ医療研究機関を含む創薬全体の臨床試験を完了するエンドユーザーアプリケーションを介して、薬物分子と患者データを共有する科学者にも当てはまるかもしれません。FMSまたはLLMに渡される機密データには、PII、財務情報、健康情報、生体認証データ、位置情報、 通信データ、オンライン行動、法的情報。リアルタイムのレンダリング、迅速な実行、エッジでの推論の場合、エンドユーザアプリケーションからストレージエンドポイントへ、オープンソースまたは独自のLLMモデルを介して、オンプレミスのデータセンターやパブリッククラウドプラットフォームにデータが移動されます。このようなすべてのシナリオで、大規模なトレーニングデータセットとその移動に依存するLLMを使用するAI運用では、データモビリティとデータ保護が不可欠です。

図4：AIとLLMの生成データパイプライン

image:gen-ai-image7.png["図4：生成型AI-LLMデータパイプライン"]

ネットアップのストレージインフラ、データ、クラウドサービスのポートフォリオには、インテリジェントなデータ管理ソフトウェアが搭載されています。

*データの準備*: LLM技術スタックの最初の柱は、従来のMLスタックからほとんど変更されていません。AIパイプラインでのデータの前処理は、トレーニングや微調整の前にデータを正規化してクレンジングするために必要です。この手順には、Amazon S3階層の形式で格納されている場所、またはオンプレミスのストレージシステム（ファイルストアやNetApp StorageGRIDなどのオブジェクトストア）にある場所にデータを取り込むためのコネクタが含まれます。

* NetApp®ONTAP *は、データセンターとクラウドにおけるネットアップの重要なストレージ・ソリューションの基盤となる基盤テクノロジです。ONTAPには、サイバー攻撃に対するランサムウェアの自動保護、組み込みのデータ転送機能、オンプレミス、ハイブリッド、NAS、SAN、オブジェクトのマルチクラウドなど、さまざまなアーキテクチャ向けのStorage Efficiency機能など、データの管理と保護に関するさまざまな機能が搭載されています。 また、LLM環境のSoftware-Defined Storage（SDS）の状況についても説明します。

* NetApp®ONTAP AI®*は、ディープラーニングモデルのトレーニングに最適です。NetApp®ONTAP®は、ONTAPストレージクラスタとNVIDIA DGXコンピューティングノードを使用するNetAppのお客様向けに、NFS over RDMAを使用してNVIDIA GPU Direct Storage™をサポートします。ストレージからメモリへのソースデータセットの読み取りと処理を何度も実行できるコスト効率に優れたパフォーマンスにより、インテリジェンスが強化され、LLMへのトレーニング、微調整、拡張アクセスが可能になります。

* NetApp®FlexCache®*は、ファイル配信を簡素化し、アクティブに読み取られたデータのみをキャッシュするリモートキャッシュ機能です。これは、LLMのトレーニング、再トレーニング、微調整に役立ち、リアルタイムレンダリングやLLM推論などのビジネス要件を持つお客様に価値を提供します。

* NetApp®SnapMirror *は、任意の2つのONTAPシステム間でボリュームSnapshotをレプリケートするONTAP機能です。この機能により、エッジからオンプレミスのデータセンターやクラウドへのデータ転送が最適化されます。お客様がエンタープライズデータを含むRAGを使用してクラウドで生成型AIを開発したい場合は、SnapMirrorを使用して、オンプレミスクラウドとハイパースケーラクラウド間で安全かつ効率的にデータを移動できます。変更のみを効率的に転送し、帯域幅を節約し、レプリケーションを高速化するため、FMSまたはLLMのトレーニング、再トレーニング、微調整の運用中に不可欠なデータ移動機能を提供します。

* NetApp®SnapLock *は、ONTAPベースのストレージシステムでデータセットのバージョンを変更できないディスク機能を提供します。マイクロコアアーキテクチャは、FPolicy™ゼロトラストエンジンを使用して顧客データを保護するように設計されています。NetAppは、攻撃者が特にリソースを消費する方法でLLMとやり取りするときにサービス拒否(DoS)攻撃に対抗することで、顧客データの可用性を確保します。

* NetApp®Cloud Data Sense *は、エンタープライズデータセットに存在する個人情報の特定、マッピング、分類、ポリシーの制定、オンプレミスまたはクラウドのプライバシー要件への対応、セキュリティ体制の改善、規制への準拠を支援します。

* Cloud Data Senseを基盤とするNetApp®BlueXP™*分類。お客様は、データ資産全体にわたってデータのスキャン、分析、分類、対処、セキュリティリスクの検出、ストレージの最適化、クラウド導入の高速化を自動で実行できます。統合されたコントロールプレーンを介してストレージとデータサービスを統合し、GPUインスタンスをコンピューティングに使用し、ハイブリッドマルチクラウド環境をコールドストレージの階層化やアーカイブとバックアップに使用できます。

* NetAppファイル-オブジェクトの二重性*。NetApp ONTAPを使用すると、NFSとS3に対するデュアルプロトコルアクセスが可能になります。この解決策を使用すると、Amazon AWS SageMakerノートブックのNFSデータに、NetApp Cloud Volumes ONTAPのS3バケットを介してアクセスできます。これにより、NFSとS3の両方のデータを共有できるため、異種データソースへの簡単なアクセスが必要なお客様に柔軟性が提供されます。たとえば、SageMaker上のMetaのLlama 2テキスト生成モデルのようなFMSを微調整し、ファイルオブジェクトバケットにアクセスできます。

* NetApp®Cloud Sync *サービスは、クラウドまたはオンプレミスの任意のターゲットにデータを移行するシンプルで安全な方法を提供します。Cloud Syncは、オンプレミスやクラウドのストレージ、NASストア、オブジェクトストア間でデータをシームレスに転送して同期します。

* NetApp XCP *は、Any-to-NetAppおよびネットアップ間のデータ移行を高速かつ信頼性の高い方法で実現するクライアントソフトウェアです。XCPは、Hadoop HDFSファイルシステムからONTAP NFS、S3、またはStorageGRIDに一括データを効率的に移動する機能も提供し、XCPファイル分析によってファイルシステムを可視化できます。

* NetApp®DataOps Toolkit *は、データサイエンティスト、DevOps、データエンジニアがさまざまなデータ管理タスクを簡単に実行できるPythonライブラリです。ハイパフォーマンスなスケールアウトNetAppストレージを基盤とするデータボリュームやJupyterLabワークスペースのプロビジョニング、クローニング、スナップショット作成など、さまざまなデータ管理タスクをほぼ瞬時に実行できます。

*ネットアップの製品セキュリティ*。LLMは、応答の中で不注意に機密データを明らかにする可能性があるため、LLMを活用するAIアプリケーションに関連する脆弱性を調査するCISOにとって懸念事項となります。OWASP(Open Worldwide Application Security Project)で概説されているように、データ中毒、データ漏えい、サービス拒否、LLM内での迅速な注入などのセキュリティ問題は、データの露出から不正アクセスへの攻撃者にサービスを提供する攻撃者に至るまで、企業に影響を与える可能性があります。データストレージの要件には、構造化データ、半構造化データ、非構造化データの整合性チェックと書き換え不可のスナップショットが含まれている必要があります。データセットのバージョン管理にはNetApp SnapshotとSnapLockが使用されています。厳格なロールベースアクセス制御（RBAC）、セキュアなプロトコル、保存中と転送中の両方のデータを保護する業界標準の暗号化を提供します。Cloud InsightsとCloud Data Senseを組み合わせることで、脅威の原因をフォレンジックで特定し、リストアするデータに優先順位を付けることができます。



=== * ONTAP AIとDGX BasePOD *

NVIDIA DGX BasePODを搭載したNetApp®ONTAP®AIリファレンスアーキテクチャは、機械学習（ML）と人工知能（AI）のワークロード向けの拡張性に優れたアーキテクチャです。LLMの重要なトレーニングフェーズでは、データは通常、データストレージからトレーニングクラスタに一定の間隔でコピーされます。このフェーズで使用されるサーバは、GPUを使用して計算処理を並列化し、大量のデータに備えています。高いGPU利用率を維持するには、物理I/O帯域幅のニーズを満たすことが非常に重要です。



=== * NVIDIA AI Enterprise搭載ONTAP AI *

NVIDIA AI Enterpriseは、NVIDIA認定システムを搭載したVMware vSphere上で動作するようにNVIDIAによって最適化、認定、サポートされている、AIとデータ分析のためのエンドツーエンドのクラウドネイティブスイートです。AIワークロードの導入、管理、拡張を簡易化し、最新のハイブリッドクラウド環境で容易に実行できます。ネットアップとVMwareを基盤とするNVIDIA AI Enterpriseは、シンプルで使いやすいパッケージで、エンタープライズクラスのAIワークロードとデータ管理を実現します。



=== * 1Pクラウドプラットフォーム*

フルマネージドのクラウドストレージサービスは、Microsoft AzureではAzure NetApp Files（ANF）、AWSではAmazon FSx for NetApp ONTAP（FSx ONTAP）、GoogleではGoogle Cloud NetApp Volume（GNCV）としてネイティブに利用できます。1Pは、ハイパフォーマンスなマネージドファイルシステムです。パブリッククラウドでデータセキュリティを強化しながら可用性の高いAIワークロードを実行し、AWS SageMaker、Azure-OpenAI Services、GoogleのVertex AIなどのクラウドネイティブMLプラットフォームでLLM / FMSを微調整できます。



== NetAppパートナー解決策スイート

NetAppは、コアデータ製品、テクノロジ、機能に加えて、強力なAIパートナーネットワークと緊密に連携し、お客様に付加価値を提供しています。

* AIシステムのNVIDIAガードレール*は、AIテクノロジーの倫理的かつ責任ある使用を保証するための保護手段として機能します。AI開発者は、特定のトピックに関するLLMベースのアプリケーションの動作を定義し、不要なトピックに関するディスカッションに参加できないようにすることができます。オープンソースのツールキットであるGuardrailsは、LLMを他のサービスにシームレスかつ安全に接続し、信頼性が高く安全で安全なLLM会話システムを構築する機能を提供します。

* Domino Data Lab *は、AI導入のどの段階にいても、ジェネレーティブAIを迅速、安全、経済的に構築し、製品化するための汎用性に優れたエンタープライズクラスのツールを提供します。DominoのエンタープライズMLOpsプラットフォームを使用すると、データサイエンティストは、好みのツールとすべてのデータを使用し、モデルをどこでも簡単にトレーニングして導入し、リスクとコスト効率に優れた方法で管理できます。すべてを1つのコントロールセンターから実行できます。

*エッジAI向けModzy *。NetApp®とModzyは提携して、画像、音声、テキスト、表など、あらゆる種類のデータに大規模なAIを提供しています。Modzyは、AIモデルを導入、統合、実行するためのMLOpsプラットフォームであり、データサイエンティストにモデル監視、ドリフト検出、説明性の機能を提供し、シームレスなLLM推論のための統合解決策を備えています。

* Run：AI *とNetAppは提携して、NetApp ONTAP AI解決策の独自機能とRun：AIクラスタ管理プラットフォームを実証し、AIワークロードのオーケストレーションを簡易化します。Spark、Ray、Dask、Rapidsの組み込み統合フレームワークにより、データ処理パイプラインを数百台のマシンに拡張するように設計されたGPUリソースを自動的に分割して結合します。



== まとめ

ジェネレーティブAIは、質の高いデータを基にモデルをトレーニングした場合にのみ効果的な結果を生み出すことができます。LLMは目覚ましいマイルストーンを達成していますが、データモビリティとデータ品質に関連する制約、設計上の課題、リスクを認識することが重要です。LLMは、異種データソースの大規模で異なるトレーニングデータセットに依存しています。モデルによって生成された不正確な結果や偏った結果は、企業と消費者の両方を危険にさらす可能性があります。これらのリスクは、データ品質、データセキュリティ、データモビリティに関連するデータ管理の課題から生じる可能性のあるLLMの制約に対応する可能性があります。NetAppは、データの急増、データモビリティ、マルチクラウド管理、AIの採用によって発生する複雑さに対応するのに役立ちます。大規模なAIインフラと効率的なデータ管理は、ジェネレーティブAIなどのAIアプリケーションの成功を定義するうえで不可欠です。コスト効率、データガバナンス、倫理的なAIプラクティスを制御しながら、企業が必要に応じて拡張する機能を犠牲にすることなく、すべての導入シナリオをカバーすることが重要です。NetAppは、お客様のAI導入の簡易化と高速化を常に支援しています。
