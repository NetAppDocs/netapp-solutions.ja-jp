---
sidebar: sidebar 
permalink: ai/aicp_rapidly_clone_a_dataset_to_create_a_data_scientist_workspace.html 
keywords: Prerequisites, DAG definition, Airflow, 
summary: このページで概説している DAG の例では、 NetApp FlexClone テクノロジを利用してデータセットボリュームを迅速かつ効率的にクローニングし、データサイエンティストや開発者のワークスペースを作成するワークフローを実装します。 
---
= データセットを迅速にクローニングして、データサイエンティストのワークスペースを作成
:hardbreaks:
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
このセクションで説明する DAG 例では、 NetApp FlexClone テクノロジを利用してデータセットボリュームを迅速かつ効率的にクローニングし、データサイエンティストや開発者のワークスペースを作成するワークフローを実装します。



=== 前提条件

この DAG を正しく機能させるには、次の前提条件を満たす必要があります。

. 「トレーサビリティとバージョン管理機能を備えたエンドツーエンドの AI トレーニングワークフローの実装」セクションの前提条件 #1 に記載されているように、 ONTAP システムの通気に接続を作成しておく必要があります。
. SSH 経由でアクセス可能で、 NetApp Trident 管理ユーティリティである「 tridentctl 」が Kubernetes クラスタを参照するようにインストールおよび設定されているホストの通気に接続を作成しておく必要があります。
+
通気で接続を管理するには、 Web サービス UI で Admin > Connections に移動します。次のスクリーンショットの例は 'tridentctl がインストールされ構成されている特定のホストの接続の作成を示しています次の値が必要です。

+
** * 接続 ID 。 * 接続の一意の名前。
** * Conn Type. * を「 SH 」に設定する必要があります。
** * ホスト。 * ホストのホスト名または IP アドレス。
** * ログイン * SSH 経由でホストにアクセスする際に使用するユーザ名。
** * パスワード。 * SSH でホストにアクセスする際に使用するパスワード。




image:aicp_imageaa3.png["エラー：グラフィックイメージがありません"]

. Kubernetes クラスタ内に、クローニングするデータセットを含むボリュームに関連付けられている既存の PersistentVolumeClaim （ PVC ；永続的ボリューム要求）がある必要があります。




=== DAG の定義

以下の Python コードの抜粋には、 DAG の例の定義が含まれています。ご使用の環境でこの DAG の例を実行する前に、「定義パラメータ」セクションのパラメータ値を変更して、ご使用の環境に合わせてください。

....
# Airflow DAG Definition: Create Data Scientist Workspace
#
# Steps:
#   1. Clone source volume
#   2. Import clone into Kubernetes using Trident
from airflow.utils.dates import days_ago
from airflow.secrets import get_connections
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.contrib.operators.ssh_operator import SSHOperator
from datetime import datetime
##### DEFINE PARAMETERS: Modify parameter values in this section to match your environment #####
## Define default args for DAG
create_data_scientist_workspace_dag_default_args = {
    'owner': 'NetApp'
}
## Define DAG details
create_data_scientist_workspace_dag = DAG(
    dag_id='create_data_scientist_workspace',
    default_args=create_data_scientist_workspace_dag_default_args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=['dev-workspace']
)
## Define volume details (change values as necessary to match your environment)
# ONTAP system details
ontapAirflowConnectionName = 'ontap_ai'  # Name of the Airflow connection that contains connection details for your ONTAP system's cluster admin account
verifySSLCert = False   # Denotes whether or not to verify the SSL cert when calling the ONTAP API
# Source volume details
sourcePvName = 'pvc-79e0855a-30a1-4f63-b34c-1029b1df49f6' # Name of Kubernetes PV corresponding to source volume
# Clone volume details (details for the new clone that you will be creating)
timestampForVolumeName = datetime.today().strftime("%Y%m%d_%H%M%S")
cloneVolumeName = 'airflow_clone_%s' % timestampForVolumeName
clonePvcNamespace = 'airflow'   # Kubernetes namespace that you want the new clone volume to be imported into
## Define tridentctl jumphost details (change values as necessary to match your environment)
tridentctlAirflowConnectionName = 'tridentctl_jumphost' # Name of the Airflow connection of type 'ssh' that contains connection details for a jumphost on which tridentctl is installed
## Define Trident details
tridentStorageClass = 'ontap-flexvol'   # Kubernetes StorageClass that you want to use when importing the new clone volume
tridentNamespace = 'trident'    # Namespace that Trident is installed in
tridentBackend = 'ontap-flexvol'    # Trident backend that you want to use when importing the new clone volume
################################################################################################
# Define function that clones a NetApp volume
def netappClone(task_instance, **kwargs) -> str :
    # Parse args
    for key, value in kwargs.items() :
        if key == 'sourcePvName' :
            sourcePvName = value
        elif key == 'verifySSLCert' :
            verifySSLCert = value
        elif key == 'airflowConnectionName' :
            airflowConnectionName = value
        elif key == 'cloneVolumeName' :
            cloneVolumeName = value
    # Install netapp_ontap package
    import sys, subprocess
    result = subprocess.check_output([sys.executable, '-m', 'pip', 'install', '--user', 'netapp-ontap'])
    print(str(result).replace('\\n', '\n'))

    # Import needed functions/classes
    from netapp_ontap import config as netappConfig
    from netapp_ontap.host_connection import HostConnection as NetAppHostConnection
    from netapp_ontap.resources import Volume, Snapshot
    from datetime import datetime
    import json
    # Retrieve ONTAP cluster admin account details from Airflow connection
    connections = get_connections(conn_id = airflowConnectionName)
    ontapConnection = connections[0]    # Assumes that you only have one connection with the specified conn_id configured in Airflow
    ontapClusterAdminUsername = ontapConnection.login
    ontapClusterAdminPassword = ontapConnection.password
    ontapClusterMgmtHostname = ontapConnection.host

    # Configure connection to ONTAP cluster/instance
    netappConfig.CONNECTION = NetAppHostConnection(
        host = ontapClusterMgmtHostname,
        username = ontapClusterAdminUsername,
        password = ontapClusterAdminPassword,
        verify = verifySSLCert
    )

    # Convert pv name to ONTAP volume name
    # The following will not work if you specified a custom storagePrefix when creating your
    #   Trident backend. If you specified a custom storagePrefix, you will need to update this
    #   code to match your prefix.
    sourceVolumeName = 'trident_%s' % sourcePvName.replace("-", "_")
    print('\nSource pv name: ', sourcePvName)
    print('Source ONTAP volume name: ', sourceVolumeName)
    # Create clone
    sourceVolume = Volume.find(name = sourceVolumeName)
    cloneVolume = Volume.from_dict({
        'name': cloneVolumeName,
        'svm': sourceVolume.to_dict()['svm'],
        'clone': {
            'is_flexclone':'true',
            'parent_volume': sourceVolume.to_dict()
        },
        'nas': {
            'path': '/%s' % cloneVolumeName
        }
    })
    response = cloneVolume.post()
    print("\nAPI Response:")
    print(response.http_response.text)
    # Retrieve clone volume details
    cloneVolume.get()
    # Convert clone volume details to JSON string
    cloneVolumeDetails = cloneVolume.to_dict()
    print("\nClone Volume Details:")
    print(json.dumps(cloneVolumeDetails, indent=2))
    # Create PVC name that resembles volume name and push as XCom for future use
    task_instance.xcom_push(key = 'clone_pvc_name', value = cloneVolumeDetails['name'].replace('_', '-'))
    # Return name of new clone volume
    return cloneVolumeDetails['name']
# Define DAG steps/workflow
with create_data_scientist_workspace_dag as dag :
    # Define step to clone source volume
    clone_source = PythonOperator(
        task_id='clone-source',
        provide_context=True,
        python_callable=netappClone,
        op_kwargs={
            'airflowConnectionName': ontapAirflowConnectionName,
            'sourcePvName': sourcePvName,
            'verifySSLCert': verifySSLCert,
            'cloneVolumeName': cloneVolumeName
        },
        dag=dag
    )
    # Define step to import clone into Kubernetes using Trident
    cloneVolumeName = "{{ task_instance.xcom_pull(task_ids='clone-source', key='return_value') }}"
    clonePvcName = "{{ task_instance.xcom_pull(task_ids='clone-source', key='clone_pvc_name') }}"
    import_command = '''cat << EOD > import-pvc-%s.yaml && tridentctl -n %s import volume %s %s -f ./import-pvc-%s.yaml && rm -f import-pvc-%s.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: %s
  namespace: %s
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: %s
EOD''' % (clonePvcName, tridentNamespace, tridentBackend, cloneVolumeName, clonePvcName, clonePvcName, clonePvcName, clonePvcNamespace, tridentStorageClass)
    import_clone = SSHOperator(
        task_id="import-clone",
        command=import_command,
        ssh_conn_id=tridentctlAirflowConnectionName
    )
    # State that the import step should be executed after the initial clone step completes
    clone_source >> import_clone
....
link:aicp_trigger_a_snapmirror_volume_replication_update.html["次の手順： SnapMirror Volume Replication Update をトリガーします"]
