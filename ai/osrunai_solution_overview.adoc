---
sidebar: sidebar 
permalink: ai/osrunai_solution_overview.html 
keywords:  
summary:  
---
= 解決策の概要
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
このセクションでは、Run：AI解決策for ONTAP AIの解決策の概要について説明します。



== NetApp ONTAP AI と AI のコントロールプレーン

ネットアップと NVIDIA が開発、検証した NetApp ONTAP AI アーキテクチャは、 NVIDIA DGX システムとネットアップのクラウド対応ストレージシステムを基盤としています。このリファレンスアーキテクチャには、 IT 組織に次のようなメリットがあります。

* 設計の複雑さを解消
* コンピューティングとストレージを個別に拡張できます
* 小規模構成から始めて、シームレスに拡張できます
* は、さまざまなパフォーマンスとに対応するさまざまなストレージオプションを提供します コストポイント


NetApp ONTAP AI は、 DGX システムと NetApp AFF A800 ストレージシステムを最先端のネットワークと緊密に統合します。NetApp ONTAP AI システムと DGX システムでは、設計の複雑さと推測に頼らず、 AI 導入を簡易化できます。お客様は小規模構成から始めて、システムを中断なく拡張できます。同時に、エッジ、コア、クラウドにわたってデータをインテリジェントに管理できます。

ネットアップの AI コントロールプレーンは、データサイエンティストとデータエンジニアを対象とした、フルスタックの AI 、 ML 、ディープラーニング（ DL ）のデータと実験管理解決策です。AI の利用が拡大するにつれて、ワークロードの拡張性やデータの可用性など、さまざまな課題が生じています。NetApp AI コントロールプレーンは、データネームスペースの迅速なクローニングを Git レポジトリと同様に行う機能や、トレーサビリティとバージョン管理のためにデータやモデルベースラインをほぼ瞬時に作成する AI トレーニングワークフローを定義して実装するなど、これらの課題に対処します。NetApp AI コントロールプレーンを使用すると、サイトやリージョン間でデータをシームレスにレプリケートし、 Jupyter Notebook ワークスペースをすばやくプロビジョニングして、大規模なデータセットにアクセスできます。



== 実行： AI ワークロードのオーケストレーション向けの AI プラットフォーム

実行： AI は、世界初の AI インフラストラクチャのオーケストレーションおよび仮想化プラットフォームを構築しました。基盤となるハードウェアからワークロードを抽象化することで、 Run ： AI は、 GPU リソースの共有プールを作成します。この共有プールを動的にプロビジョニングし、 AI ワークロードの効率的なオーケストレーションと GPU の使用の最適化を実現します。データサイエンティストは、大量の GPU パワーをシームレスに消費して研究を向上させ、加速させることができます。一方、 IT チームは、リソースのプロビジョニング、キューイング、利用率に関する一元化されたクロスサイト管理とリアルタイムの可視性を維持します。Run ： AI プラットフォームは Kubernetes を基盤として構築されているため、既存の IT ワークフローやデータサイエンスワークフローと簡単に統合できます。

Run ： AI プラットフォームには、次のようなメリットがあります。

* * イノベーションにかかる時間を短縮 * Run ： AI リソースのプール化、キューイング、優先付けのメカニズムをネットアップストレージシステムと組み合わせることで、研究者たちはインフラ管理の苦労から取り取り除かれ、データサイエンスに専念することができます。実行： AI とネットアップのお客様は、コンピューティングやデータパイプラインのボトルネックを発生させることなく、必要な数のワークロードを実行することで生産性を向上できます。
* * チームの生産性向上。 * 実行： AI 公正性アルゴリズムにより、すべてのユーザーとチームが公平なリソースを共有できるようになります。優先度の高いプロジェクトを中心としたポリシーをあらかじめ設定しておくことで、あるユーザやチームから別のユーザやチームにリソースを動的に割り当てることができ、誰もが切望した GPU リソースにタイムリーにアクセスできるようになります。
* * GPU 利用率の向上。 * 実行： AI スケジューラを使用すると、 Kubernetes での分散トレーニング用に、フラクショナルな GPU 、整数型 GPU 、複数の GPU ノードを簡単に利用できます。このように、 AI ワークロードは容量ではなくニーズに基づいて実行されます。データサイエンスチームは、 1 つのインフラでより多くの AI 実験を実行できるようになります。

