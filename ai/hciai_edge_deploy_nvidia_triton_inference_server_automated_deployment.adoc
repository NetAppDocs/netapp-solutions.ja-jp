---
sidebar: sidebar 
permalink: ai/hciai_edge_deploy_nvidia_triton_inference_server_automated_deployment.html 
keywords:  
summary:  
---
= NVIDIA Triton Inference Server の導入（自動導入）
:hardbreaks:
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


Triton Inference Server の自動導入を設定するには、次の手順を実行します。

. VI エディタを開き、 PVC YAML ファイル「 vi pvC-Triton-mmodel-repo_yaml 」を作成します。
+
....
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: triton-pvc  namespace: triton
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: ontap-flexvol
....
. PVC を作成します。
+
....
kubectl create -f pvc-triton-model-repo.yaml
....
. VI エディタを開き 'Triton Inference Server の配備を作成し 'Triton_deployment.yaml ファイルを呼び出します
+
....
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: triton-3gpu
  name: triton-3gpu
  namespace: triton
spec:
  ports:
  - name: grpc-trtis-serving
    port: 8001
    targetPort: 8001
  - name: http-trtis-serving
    port: 8000
    targetPort: 8000
  - name: prometheus-metrics
    port: 8002
    targetPort: 8002
  selector:
    app: triton-3gpu
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: triton-1gpu
  name: triton-1gpu
  namespace: triton
spec:
  ports:
  - name: grpc-trtis-serving
    port: 8001
    targetPort: 8001
  - name: http-trtis-serving
    port: 8000
    targetPort: 8000
  - name: prometheus-metrics
    port: 8002
    targetPort: 8002
  selector:
    app: triton-1gpu
  type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: triton-3gpu
  name: triton-3gpu
  namespace: triton
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton-3gpu      version: v1
  template:
    metadata:
      labels:
        app: triton-3gpu
        version: v1
    spec:
      containers:
      - image: nvcr.io/nvidia/tritonserver:20.07-v1-py3
        command: ["/bin/sh", "-c"]
        args: ["trtserver --model-store=/mnt/model-repo"]
        imagePullPolicy: IfNotPresent
        name: triton-3gpu
        ports:
        - containerPort: 8000
        - containerPort: 8001
        - containerPort: 8002
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
            nvidia.com/gpu: 3
          requests:
            cpu: "2"
            memory: 4Gi
            nvidia.com/gpu: 3
        volumeMounts:
        - name: triton-model-repo
          mountPath: /mnt/model-repo      nodeSelector:
        gpu-count: “3”
      volumes:
      - name: triton-model-repo
        persistentVolumeClaim:
          claimName: triton-pvc---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: triton-1gpu
  name: triton-1gpu
  namespace: triton
spec:
  replicas: 3
  selector:
    matchLabels:
      app: triton-1gpu
      version: v1
  template:
    metadata:
      labels:
        app: triton-1gpu
        version: v1
    spec:
      containers:
      - image: nvcr.io/nvidia/tritonserver:20.07-v1-py3
        command: ["/bin/sh", "-c", “sleep 1000”]
        args: ["trtserver --model-store=/mnt/model-repo"]
        imagePullPolicy: IfNotPresent
        name: triton-1gpu
        ports:
        - containerPort: 8000
        - containerPort: 8001
        - containerPort: 8002
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
            nvidia.com/gpu: 1
          requests:
            cpu: "2"
            memory: 4Gi
            nvidia.com/gpu: 1
        volumeMounts:
        - name: triton-model-repo
          mountPath: /mnt/model-repo      nodeSelector:
        gpu-count: “1”
      volumes:
      - name: triton-model-repo
        persistentVolumeClaim:
          claimName: triton-pvc
....
+
例として、 2 つの配置がここに作成されます。最初の導入では、 3 つの GPU を使用し、レプリカが 1 に設定されたポッドがスピンアップします。もう 1 つの導入環境では、各ポッドが 1 つの GPU を使用してスピンアップし、レプリカは 3 に設定されます。要件に応じて、 GPU 割り当てとレプリカ数を変更できます。

+
どちらの環境でも、前に作成した PVC が使用され、この永続的ストレージがモデルリポジトリとして Triton 推論サーバに提供されます。

+
導入ごとに、 LoadBalancer タイプのサービスが作成されます。Triton Inference Server には、アプリケーションネットワーク内のロードバランサ IP を使用してアクセスできます。

+
nodeSelector は、両方の導入で、問題なく必要な数の GPU を取得できるようにするために使用されます。

. K8 ワーカーノードにラベルを付けます。
+
....
kubectl label nodes hci-ai-k8-worker-01 gpu-count=3
kubectl label nodes hci-ai-k8-worker-02 gpu-count=1
....
. 導入環境を作成
+
....
kubectl apply -f triton_deployment.yaml
....
. LoadBalancer サービスの外部 LPS をメモします。
+
....
kubectl get services -n triton
....
+
想定される出力例を次に示します。

+
image:hciaiedge_image21.png["エラー：グラフィックイメージがありません"]

. 導入環境から作成したポッドのいずれかに接続します。
+
....
kubectl exec -n triton --stdin --tty triton-1gpu-86c4c8dd64-545lx -- /bin/bash
....
. モデルリポジトリの例を使用して、モデルリポジトリをセットアップします。
+
....
git clone
cd triton-inference-server
git checkout r20.07
....
. 不足しているモデル定義ファイルを取得します。
+
....
cd docs/examples
./fetch_models.sh
....
. すべてのモデルをモデルリポジトリの場所にコピーするか ' 使用する特定のモデルのみにコピーします
+
....
cp -r model_repository/resnet50_netdef/ /mnt/model-repo/
....
+
この解決策では 'resnet50_netdef モデルのみが例としてモデルリポジトリにコピーされます

. Triton Inference Server のステータスを確認します。
+
....
curl -v <<LoadBalancer_IP_recorded earlier>>:8000/api/status
....
+
想定される出力例を次に示します。

+
....
curl -v 172.21.231.132:8000/api/status
*   Trying 172.21.231.132...
* TCP_NODELAY set
* Connected to 172.21.231.132 (172.21.231.132) port 8000 (#0)
> GET /api/status HTTP/1.1
> Host: 172.21.231.132:8000
> User-Agent: curl/7.58.0
> Accept: */*
>
< HTTP/1.1 200 OK
< NV-Status: code: SUCCESS server_id: "inference:0" request_id: 9
< Content-Length: 1124
< Content-Type: text/plain
<
id: "inference:0"
version: "1.15.0"
uptime_ns: 377890294368
model_status {
  key: "resnet50_netdef"
  value {
    config {
      name: "resnet50_netdef"
      platform: "caffe2_netdef"
      version_policy {
        latest {
          num_versions: 1
        }
      }
      max_batch_size: 128
      input {
        name: "gpu_0/data"
        data_type: TYPE_FP32
        format: FORMAT_NCHW
        dims: 3
        dims: 224
        dims: 224
      }
      output {
        name: "gpu_0/softmax"
        data_type: TYPE_FP32
        dims: 1000
        label_filename: "resnet50_labels.txt"
      }
      instance_group {
        name: "resnet50_netdef"
        count: 1
        gpus: 0
        gpus: 1
        gpus: 2
        kind: KIND_GPU
      }
      default_model_filename: "model.netdef"
      optimization {
        input_pinned_memory {
          enable: true
        }
        output_pinned_memory {
          enable: true
        }
      }
    }
    version_status {
      key: 1
      value {
        ready_state: MODEL_READY
        ready_state_reason {
        }
      }
    }
  }
}
ready_state: SERVER_READY
* Connection #0 to host 172.21.231.132 left intact
....


link:hciai_edge_deploy_the_client_for_triton_inference_server_automated_deployment.html["次のステップ： Triton Inference Server （自動導入）用のクライアントの導入"]
