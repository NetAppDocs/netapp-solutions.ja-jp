---
sidebar: sidebar 
permalink: ai/vector-database-use-cases.html 
keywords: vector database 
summary: ユースケースベクターデータベース解決策for NetApp 
---
= Vectorデータベースのユースケース
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
このセクションでは、NetAppベクターデータベース解決策のユースケースの概要を説明します。



== Vectorデータベースのユースケース

このセクションでは、大規模言語モデルを使用したRetrieval Augmented GenerationとNetApp ITチャットボットの2つのユースケースについて説明します。



=== LLM（Large Language Models）を使用したRAG（Retrieval Augmented Generation）

....
Retrieval-augmented generation, or RAG, is a technique for enhancing the accuracy and reliability of Large Language Models, or LLMs, by augmenting prompts with facts fetched from external sources. In a traditional RAG deployment, vector embeddings are generated from an existing dataset and then stored in a vector database, often referred to as a knowledgebase. Whenever a user submits a prompt to the LLM, a vector embedding representation of the prompt is generated, and the vector database is searched using that embedding as the search query. This search operation returns similar vectors from the knowledgebase, which are then fed to the LLM as context alongside the original user prompt. In this way, an LLM can be augmented with additional information that was not part of its original training dataset.
....
NVIDIA Enterprise RAG LLM Operatorは、エンタープライズにRAGを実装するための便利なツールです。このオペレータを使用して、完全なRAGパイプラインを展開できます。RAGパイプラインは、ナレッジベースの埋め込みを格納するためのベクターデータベースとしてMilvusまたはpgvectoのいずれかを利用するようにカスタマイズできます。詳細については、ドキュメントを参照してください。

....
NetApp has validated an enterprise RAG architecture powered by the NVIDIA Enterprise RAG LLM Operator alongside NetApp storage. Refer to our blog post for more information and to see a demo. Figure 1 provides an overview of this architecture.
....
図1）NVIDIA NemoマイクロサービスとNetAppを基盤とするエンタープライズRAG

image:RAG_nvidia_nemo.png["入力/出力ダイアログを示す図、または書き込まれた内容を表す図"]



=== NetApp ITチャットボットのユースケース

ネットアップのチャットボットは、ベクターデータベースのもう1つのリアルタイムユースケースとして機能します。この場合、NetAppプライベートOpenAIサンドボックスは、ネットアップの社内ユーザからのクエリを管理するための、効果的でセキュアで効率的なプラットフォームを提供します。厳格なセキュリティプロトコル、効率的なデータ管理システム、高度なAI処理機能を組み込むことで、SSO認証を介して組織内のユーザーの役割と責任に基づいて、ユーザーに対する高品質で正確な応答を保証します。このアーキテクチャは、高度なテクノロジーを統合して、ユーザー重視のインテリジェントなシステムを構築する可能性を強調しています。

image:netapp_chatbot.png["入力/出力ダイアログを示す図、または書き込まれた内容を表す図"]

ユースケースは、主に4つのセクションに分けることができます。



==== ユーザ認証と検証：

* ユーザークエリは、最初にNetAppシングルサインオン(SSO)プロセスを実行して、ユーザーのIDを確認します。
* 認証に成功すると、システムはVPN接続をチェックして、安全なデータ転送を保証します。




==== データ転送と処理：

* VPNが検証されると、データはNetAIChatまたはNetAICreate Webアプリケーションを介してMariaDBに送信されます。MariaDBは、ユーザデータの管理と保存に使用される高速で効率的なデータベースシステムです。
* 次に、MariaDBはその情報をNetApp Azureインスタンスに送信し、AzureインスタンスはユーザデータをAI処理ユニットに接続します。




==== OpenAIとコンテンツフィルタリングとの相互作用：

* Azureインスタンスは、ユーザーの質問をコンテンツフィルタリングシステムに送信します。このシステムはクエリをクリーンアップし、処理の準備を行います。
* その後、クリーンアップされた入力がAzure OpenAIベースモデルに送信され、入力に基づいて応答が生成されます。




==== 応答の生成と管理：

* ベースモデルからの応答は、正確でコンテンツ基準を満たしていることを確認するために最初にチェックされます。
* チェックに合格すると、応答がユーザに返送されます。このプロセスにより、ユーザーはクエリに対して明確で正確で適切な回答を受け取ることができます。

