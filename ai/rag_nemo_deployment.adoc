---
sidebar: sidebar 
permalink: ai/rag_nemo_deployment.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: NetAppを使用したエンタープライズRAG - Nemoマイクロサービスの導入 
---
= Nemoマイクロサービスの導入
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
このセクションでは、NetAppストレージと一緒にNVIDIA Nemoマイクロサービスを導入するために実行する必要があるタスクについて説明します。NVIDIA Nemoマイクロサービスは、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["NVIDIA Enterprise RAG LLMオペレータ"]。



== 前提条件

このセクションで説明する手順を実行する前に、次のタスクがすでに実行されていることを前提としています。

* すでに稼働中のKubernetesクラスタがあり、NVIDIA Enterprise RAG LLM OperatorでサポートされているバージョンのKubernetesを実行している。サポートされているKubernetesバージョンのリストについては、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html["Rag LLM Operatorのマニュアル。"] このKubernetesクラスタは、オンプレミスでもクラウドでもかまいません。
* Kubernetesクラスタには、NVIDIA Enterprise RAG LLM OperatorでサポートされるGPUが少なくとも3つ含まれています。サポートされているGPUの一覧については、を参照してください。 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html["Rag LLM Operatorのマニュアル。"]
* KubernetesクラスタにNetApp Astra Tridentをインストールして設定しておきます。Astra Tridentの詳細については、 link:https://docs.netapp.com/us-en/trident/index.html["Astra Trident のドキュメント"]。この解決策は、Tridentでサポートされる任意のNetApp物理ストレージアプライアンス、Software-Definedインスタンス、クラウドサービスと互換性があります。




== NVIDIA Enterprise RAG LLMオペレータを使用してNVIDIA Nemoマイクロサービスを導入する

. NVIDIA GPU OperatorがKubernetesクラスタにインストールされていない場合は、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-nvidia-gpu-operator["Rag LLM Operatorのマニュアル。"]
. NVIDIA Enterprise RAG LLM Operatorをインストールするには、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-rag-llm-operator["Rag LLM Operatorのマニュアル。"]
. NVIDIA Enterprise RAG LLMオペレータを使用してRAGパイプラインを作成するには、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/pipelines.html["Rag LLM Operatorのマニュアル。"]
+
** StorageClassを指定する場合は、Astra Tridentを使用するStorageClassを指定してください。
** デフォルトでは、RAGパイプラインは新しいpgvectorデータベースをデプロイし、RAGデプロイのベクターストア/ナレッジベースとして機能します。代わりに既存のpgvectorまたはMilvusインスタンスを使用する場合は、 link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/vector-database.html["Rag LLM Operatorのマニュアル。"] NetAppを使用したベクターデータベースの実行について詳しくは、 link:https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["NetApp VECTOR DATABASE解決策のドキュメント"]



