---
sidebar: sidebar 
permalink: ai/hciai_edge_design_considerations.html 
keywords:  
summary:  
---
= 設計上の考慮事項
:hardbreaks:
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== ネットワーク設計

NetApp HCI トラフィックの処理に使用するスイッチでは、導入を成功させるために特別な設定が必要になります。

物理的なケーブル配線とスイッチの詳細については、『 NetApp HCI ネットワークセットアップガイド』を参照してください。この解決策は、コンピューティングノードにケーブル 2 本の設計を使用します。オプションとして、 6 ノードのケーブル設計でコンピューティングノードを構成したり、コンピューティングノードの導入オプションを指定したりできます。

の図 link:hciaiedge_architecture.html["アーキテクチャ"] は、この NetApp HCI 解決策のネットワークトポロジを示しており、ケーブルを 2 本使用してコンピューティングノードを構成しています。



== 設計を計算（ Compute Design

NetApp HCI コンピューティングノードには、ハーフ幅とフル幅の 2 つのフォームファクタがあり、 1 RU と 2 RU の 2 つのラックユニットサイズがあります。この解決策で使用される 410c ノードはハーフ幅と 1 RU であり、最大 4 つのこのようなノードを収容できるシャーシに格納されます。この解決策で使用されるもう 1 つのコンピューティングノードは、全幅、 1 RU の H615c です。H410c ノードには Intel Skylake プロセッサが、 H615c ノードには第 2 世代の Intel Cascade Lake プロセッサが搭載されています。H615c ノードには NVIDIA GPU を追加できます。各ノードでは最大 3 つの NVIDIA Tesla T4 16GB GPU をホストできます。

H615c ノードは、 NetApp HCI 向けの最新のコンピューティングノードであり、 GPU をサポートする 2 番目のシリーズです。GPU をサポートする最初のモデルは、 H615c ノード（全幅、 2RU ）です。 2 つの NVIDIA Tesla M10 GPU をサポートできます。

この解決策では、次の利点を考慮して、 H610c ノードよりも H615c ノードが推奨されます。

* データセンターの設置面積を削減し、エッジ環境に不可欠
* 高速化を目的とした新世代の GPU のサポート 推論
* 消費電力の削減
* 熱放散の削減




== NVIDIA T4 GPU の略語

推論のリソース要件は、トレーニングワークロードの要件とはまったく異なります。実際、最新のハンドヘルドデバイスは、 GPU などの強力なリソースを使用せずに少量の推論を処理できます。しかし、高度な並列化と大量の入力バッチサイズの対象となる一方で、非常に低い推論レイテンシを必要とするさまざまなアプリケーションを扱うミッションクリティカルなアプリケーションやデータセンターでは、 GPU が推論時間の短縮とアプリケーションのパフォーマンス向上に重要な役割を果たします。

NVIDIA Tesla T4 は、 Turing アーキテクチャを基盤とする x16 PCIe Gen3 、 1 スロットロープロファイル GPU です。T4 GPU は、画像の分類やタギング、ビデオ分析、自然言語処理、自動音声認識、インテリジェント検索などのアプリケーションにまたがる汎用性に優れた推論アクセラレーションを提供します。エンタープライズソリューションやエッジデバイスでは、 Tesla T4 の幅広い推論機能を使用できます。

これらの GPU は、消費電力が低く、 PCIe フォームファクタが小さいため、エッジインフラへの導入に最適です。T4 GPU のサイズにより、 2 つの T4 GPU を 2 つのダブルスロットのフルサイズ GPU と同じスペースに設置することができます。T4S はコンパクトですが、 16GB のメモリを搭載しています。大規模な ML モデルをサポートしたり、複数の小規模モデルで同時に推論を実行したりできます。

Turing ベースの T4 GPU は、 Tensor コアの強化バージョンを備えており、推論のためのさまざまな精度をサポートします。 FP32 、 FP16 、 INT8 、 int4GPU に搭載されている CUDA コアの数は 2 、 560 個、 Tensor コアの数は 320 個で、 INT8 の 1 秒あたり 130 Tera 処理数（ TOPS ）、 INT4 推論パフォーマンスの最大値は 260 TOPS です。CPU ベースの推論と比較すると、新しい Turing Tensor コアを基盤とする Tesla T4 が提供する推論パフォーマンスは最大 40 倍に向上します。

Turing Tensor コアは、ニューラルネットワークのトレーニングと推論の機能の中核をなす行列と行列の乗算を高速化します。特に、推論の計算において有用で関連性の高い情報を推論し、所定の入力に基づいてトレーニング済みのディープニューラルネットワークによって提供できることが重要です。

Turing GPU アーキテクチャは、 Volta アーキテクチャで導入された高度な Multi-Process Service （ MPS ）機能を継承しています。Pascal ベースの Tesla GPU と比較すると、 Tesla T4 の MPS は小さいバッチサイズに対する推論パフォーマンスが向上し、起動レイテンシが低減され、 QoS が向上し、多数のクライアントの同時要求の処理が可能になります。

NVIDIA T4 GPU は、すべての AI フレームワークをサポートする NVIDIA AI 推論プラットフォームの一部です。包括的なツールと統合を提供することで、高度な AI の開発と導入を大幅に簡易化します。



== ストレージ設計： Element ソフトウェア

NetApp Element ソフトウェアは、 NetApp HCI システムのストレージを強化します。スケールアウトの柔軟性とアプリケーションのパフォーマンス保証を通じて、即応性に優れた自動化を実現し、新しいサービスの提供を加速します。

システムを停止することなくストレージノードを 1 つずつ追加でき、ストレージリソースをアプリケーションですぐに使用できます。システムに新しいノードを追加するたびに、使用可能なプールに正確なパフォーマンスと容量が追加されます。データは、バックグラウンドでクラスタ内のすべてのノードに自動的に負荷分散されるため、システムが拡大しても利用率は均一に維持されます。

Element ソフトウェアは NetApp HCI システムをサポートしているため、各ワークロードに対して QoS を保証することで、複数のワークロードを快適にホストできます。各ワークロードの最小値、最大値、バースト値を細かく設定してパフォーマンスをきめ細かく制御できるため、アプリケーションのパフォーマンスを保護しながら、十分に計画された統合を行うことができます。また、パフォーマンスと容量が切り離され、各ボリュームに特定の容量とパフォーマンスを割り当てることができます。これらの仕様は、データアクセスを中断することなく動的に変更できます。

次の図に示すように、 Element ソフトウェアは NetApp ONTAP と統合され、異なるストレージオペレーティングシステムを実行するネットアップストレージシステム間でデータを移動できるようにします。NetApp SnapMirror テクノロジを使用すると、 Element ソフトウェアと ONTAP の間でデータを移動できます。Element では、 NetApp Cloud Volumes ONTAP と統合することで同じテクノロジを使用してクラウドとの接続を実現します。 Element は、エッジからコア、複数のパブリッククラウドサービスプロバイダにデータを移動できるようにします。

この解決策では、 Element ベースのストレージによって、 NetApp HCI システムでワークロードとアプリケーションを実行するために必要なストレージサービスが提供されます。

image:hciaiedge_image4.png["エラー：グラフィックイメージがありません"]



== ストレージ設計： ONTAP Select

ネットアップで NetApp HCI は、 ONTAP Select を基盤に、 Software-Defined Data Storage サービスモデルを導入しています。NetApp HCI 機能をベースに構築されており、豊富なファイルサービスとデータサービスを HCI プラットフォームに追加すると同時に、データファブリックを拡張します。

ONTAP Select はこの解決策を実装するためのオプションコンポーネントですが、データ収集、保護、モビリティなど、さまざまなメリットがあります。 これは、 AI データライフサイクル全体のコンテキストに非常に有効です。データの取り込み、収集、トレーニング、導入など、データ処理に関する日々の課題を簡易化します。 階層化を実現できます。

image:hciaiedge_image5.png["エラー：グラフィックイメージがありません"]

ONTAP Select は VMware 上で VM として実行でき、専用の FAS プラットフォームで実行されている場合でも、次のような ONTAP の機能のほとんどを利用できます。

* NFS と CIFS がサポートされます
* NetApp FlexClone テクノロジ
* NetApp FlexCache テクノロジ
* NetApp ONTAP FlexGroup ボリューム
* NetApp SnapMirror ソフトウェア


ONTAP Select を使用して FlexCache 機能を活用すると、次の図に示すように、バックエンドの元のボリュームから頻繁に読み取られるデータをキャッシュすることで、データ読み取りのレイテンシを低減できます。多くの並列化が行われているハイエンドな推論アプリケーションの場合、推論プラットフォーム全体に同じモデルの複数のインスタンスが導入されるため、同じモデルを複数回読み取ることができます。トレーニングを受けたモデルの新しいバージョンは、目的のモデルが元のボリュームまたはソースボリュームで使用可能であることを確認することで、推論プラットフォームにシームレスに導入できます。

image:hciaiedge_image6.png["エラー：グラフィックイメージがありません"]



== NetApp Trident

NetApp Trident は、主要なネットアップストレージプラットフォームすべてにわたってストレージリソースを管理できる、オープンソースの動的なストレージオーケストレーションツールです。Kubernetes とネイティブに統合されるため、永続ボリューム（ PVS ）をネイティブの Kubernetes インターフェイスや構成要素を使用してオンデマンドでプロビジョニングできます。Trident を使用すると、マイクロサービスやコンテナ化されたアプリケーションで、 QoS 、 Storage Efficiency 、クローニングなどのエンタープライズクラスのストレージサービスを利用して、アプリケーションの永続的ストレージのニーズを満たすことができます。

コンテナは、アプリケーションのパッケージ化と導入の最も一般的な方法の 1 つです。 Kubernetes は、コンテナ化されたアプリケーションをホストする最も一般的なプラットフォームの 1 つです。この解決策では、推論プラットフォームは Kubernetes インフラ上に構築されます。

Trident は現在、次のプラットフォームにわたってストレージオーケストレーションをサポートしています。

* ONTAP ： NetApp AFF 、 FAS 、 Select
* Element ソフトウェア： NetApp HCI と NetApp SolidFire オールフラッシュストレージ
* NetApp SANtricity ソフトウェア： E シリーズと EF シリーズ
* Cloud Volumes ONTAP
* Azure NetApp Files の特長
* NetApp Cloud Volumes Service ： AWS と Google Cloud


Trident は、複数のストレージプラットフォームにわたってだけでなく、エッジからコア、クラウドまで、 AI データライフサイクル全体にわたってストレージオーケストレーションを可能にするシンプルで強力なツールです。

Trident を使用すると、トレーニング済みモデルを構成するネットアップの Snapshot コピーから PV をプロビジョニングできます。次の図に、 Trident のワークフローを示します。このワークフローでは、既存の Snapshot コピーを参照して、永続的ボリューム要求（ PVC ）が作成されます。Trident では、この Snapshot コピーを使用してボリュームを作成します。

image:hciaiedge_image7.png["エラー：グラフィックイメージがありません"]

この方法で Snapshot コピーからトレーニング済みモデルを導入すると、強力なモデルバージョン管理がサポートされます。新しいバージョンのモデルをアプリケーションに導入し、異なるバージョンのモデル間で推論を切り替えるプロセスを簡易化します。



== NVIDIA DeepOps のことです

NVIDIA DeepOps は、 Ansible スクリプトのモジュラ型コレクションで、 Kubernetes インフラの導入を自動化する際に使用できます。Kubernetes クラスタの導入を自動化する導入ツールは複数あります。この解決策では、 Kubernetes インフラの導入だけでなく、必要な GPU ドライバ、 NVIDIA Container Runtime for Docker （ NVIDIA - docker2 ）など、 GPU によって高速化された作業に必要なその他の依存関係もインストールされるため、 DeepOps が最適な選択肢です。また、 NVIDIA GPU のベストプラクティスをカプセル化し、必要に応じて個々のコンポーネントとしてカスタマイズしたり実行したりできます。

DeepOps 内部では Kubespray を使用して Kubernetes を導入し、 DeepOps にサブモジュールとして含まれています。そのため、 Kubespray を使用して、ノードの追加、ノードの削除、クラスタのアップグレードなど、 Kubernetes での一般的なクラスタ管理操作を行う必要があります。

Metallb を使用するソフトウェアベースの L2 LoadBalancer と NGINX ベースの入力コントローラも、 DeepOps で使用可能なスクリプトを使用して、この解決策の一部として導入されます。

この解決策では、 3 つの Kubernetes マスターノードが VM として導入され、 NVIDIA Tesla T4 GPU を搭載した 2 つの H615c コンピューティングノードが Kubernetes ワーカーノードとしてセットアップされます。



== NVIDIA GPU Operator （ NVIDIA GPU オペレータ）

GPU Operator は、 GPU サポート用の NVIDIA k8s-device-plugin を導入し、 NVIDIA ドライバをコンテナとして実行します。Kubernetes オペレータフレームワークをベースにしており、 GPU のプロビジョニングに必要なすべての NVIDIA ソフトウェアコンポーネントの管理を自動化できます。コンポーネントには、 NVIDIA ドライバ、 GPU 用の Kubernetes デバイスプラグイン、 NVIDIA コンテナランタイム、自動ノードラベルが含まれ、 Kubernetes Node Feature Discovery と連携して使用されます。

GPU 演算子は、の重要なコンポーネントです https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/["NVIDIA EGX"^] 大規模なハイブリッドクラウドとエッジの運用を可能にし、効率的に行えるように設計された、ソフトウェア定義型プラットフォーム。特に、 Kubernetes クラスタで GPU ベースのワーカーノードを追加でプロビジョニングし、基盤となるソフトウェアコンポーネントのライフサイクルを管理する場合など、迅速な拡張が必要になる場合に便利です。GPU Operator は NVIDIA ドライバを含むすべてのコンポーネントをコンテナとして実行するため、管理者はコンテナの起動や停止だけでさまざまなコンポーネントを簡単にスワップできます。



== NVIDIA Triton 推論サーバ

NVIDIA Triton Inference Server （ Triton Server ）：本番データセンターへの AI 推論ソリューションの導入を簡易化します。このマイクロサービスは、本番環境のデータセンターでの推論のために特別に設計されたものです。GPU 利用率を最大限に高め、 Docker と Kubernetes を使用して DevOps 環境にシームレスに統合します。

Triton サーバは、 AI 推論のための共通の解決策を提供します。そのため、研究者は質の高いトレーニング済みモデルの作成に集中でき、 DevOps エンジニアは導入に集中できます。開発者は、 AI を利用する各アプリケーションのプラットフォームを再設計することなく、アプリケーションに集中できます。

ここでは、 Triton サーバの主な機能をいくつか紹介します。

* * 複数のフレームワークをサポート。 * Triton サーバーはモデルの混在を処理でき、モデルの数はシステムディスクとメモリリソースによってのみ制限されます。サポートできるのは、 TensorRT 、 TensorFlow GraphDef 、 TensorFlow SavedModel 、 ONNX 、 PyTorch 、 および Caffe2 NetDef モデル形式。
* * 同時モデル実行。* 複数のモデルまたは同じモデルの複数のインスタンスを 1 つの GPU で同時に実行できます。
* * マルチ GPU のサポート。 * Triton Server は、 1 つ以上の GPU で複数のモデルの推論を有効にすることで、 GPU 利用率を最大限に高めることができます。
* * バッチ処理をサポート。 * Triton サーバーは、入力のバッチ要求を受け付け、対応する出力バッチで応答できます。推論サーバでは、個々の推論要求をまとめて推論のスループットを向上するスケジュール設定アルゴリズムとバッチ処理アルゴリズムを複数サポートしています。バッチ処理アルゴリズムはステートレスアプリケーションとステートフルアプリケーションの両方で使用でき、適切に使用する必要があります。こうしたスケジュール設定とバッチ処理の決定は、推論を要求しているクライアントに対して透過的に行われます。
* * アンサンブルのサポート * アンサンブルとは、入力と出力のテンソルが接続された複数のモデルを持つパイプラインです。推論要求はアンサンブルに対して行うことができます。これにより、パイプライン全体が実行されます。
* * メトリクス * メトリクスとは、 GPU 利用率、サーバのスループット、サーバのレイテンシ、および自動スケーリングとロードバランシングの健全性に関する詳細です。


NetApp HCI は、複数のワークロードとアプリケーションをホストできるハイブリッドマルチクラウドインフラです。 Triton Inference Server は、複数のアプリケーションの推論の要件をサポートするための十分な機能を備えています。

この解決策では、 Triton サーバは導入ファイルを使用して Kubernetes クラスタに導入されます。この方法では、 Triton Server のデフォルト設定を必要に応じて上書きおよびカスタマイズできます。また、 Triton サーバは、 HTTP または gRPC エンドポイントを使用する推論サービスも提供します。これにより、リモートクライアントは、サーバで管理されている任意のモデルに対して推論を要求できます。

永続ボリュームは、 NetApp Trident を介して Triton Inference Server を実行するコンテナに提示され、この永続ボリュームは推論サーバのモデルリポジトリとして構成されます。

Triton Inference Server は、 Kubernetes 導入ファイルを使用してさまざまなリソースセットを導入し、各サーバインスタンスにはシームレスな拡張性を実現するロードバランサフロントエンドが提供されます。また、このアプローチは、推論ワークロードにリソースを割り当てることができる柔軟性と簡易性も示しています。

link:hciai_edge_deployment_steps.html["次の例：エッジでの NetApp HCI と AI 推論の導入"]
