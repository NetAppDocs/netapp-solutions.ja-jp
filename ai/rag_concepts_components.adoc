---
sidebar: sidebar 
permalink: ai/rag_concepts_components.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: NetAppを使用したエンタープライズRAG -概念とコンポーネント 
---
= コンセプトとコンポーネント
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== ジェネレーティブAI

ジェネレーティブAIのようなAIシステムは、大規模なデータセットに教師なしまたは自動教師なしの機械学習を適用することで設計されています。特定のデータセットについて予測する従来の機械学習モデルとは異なり、生成AIモデルは、ユーザーのプロンプトに応じてテキスト、コード、画像、ビデオ、オーディオなどの新しいコンテンツを生成できます。このため、ジェネレーティブAIシステムの機能も、使用するデータのモダリティやタイプに基づいて分類されます。これらはunimodalまたはmultimodalのいずれかである。ユニモーダルシステムでは、入力のタイプは1つだけです（例：テキストのみまたは画像のみ）。一方、マルチモーダルシステムでは複数のタイプの入力（例：テキスト、画像、音声）を使用して、さまざまなモダリティでコンテンツを同時に理解し、生成します。本質的に、生成型AIは、企業がコンテンツを作成し、新しい設計コンセプトを生成し、既存データから価値を引き出す方法を変革しています。



=== Large Language Models（LLM;ラージ言語モデル）

LLMは、膨大な量のデータに対して事前にトレーニングされたディープラーニングモデルであり、テキストを認識して生成することができます。LLMは言語に焦点を当てた生成AIのサブセットとして始まったが、そのような区別はマルチモーダルLLMが出現し続けるにつれて徐 々 に薄れてきている。LLMの基礎となるトランスは、RNNやCNN以外の新しいネットワークアーキテクチャを導入します。エンコーダとデコーダで構成されるニューラルネットワークのセットがあり、テキストシーケンスから意味を抽出し、単語間の関係を理解するのに役立ちます。LLMは、自然な人間の言語に応答し、非構造化された質問を回答にデータ分析を使用することができます。しかし、LLMは取り込んだデータほど信頼性が高くないため、ガベージインやガベージアウトの課題に起因する幻覚が発生しやすくなります。LLMに誤った情報が入力されると、ユーザからの問い合わせに応じて不正確な出力が生成される可能性があります。これは、LLMが構築しているストーリーに合わせたものです。私たちのエビデンスに基づく研究では、AIエンジニアはこれらの幻覚に対抗するためにさまざまな方法に依存していることが示唆されています。



=== Retrieval Augmented Generation（RAG）

LLMは大量のデータでトレーニングされますが、データではトレーニングされません。Ragは、LLMがすでにアクセスできるデータにデータを追加することで、この問題を解決します。RAGを使用すると、LLMの機能を活用してデータをトレーニングし、情報を取得してそれを使用して生成AIユーザーにコンテキスト情報を提供できます。RAGは機械学習技術であり、幻覚を減らし、LLMの有効性と信頼性を向上させ、AIアプリケーションの開発を加速させ、エンタープライズ検索エクスペリエンスを強化するのに役立つアーキテクチャアプローチです。



=== ラガス

RAGパイプラインの構築に役立つ既存のツールやフレームワークはありますが、それを評価してパイプラインのパフォーマンスを数値化するのは難しい場合があります。ここでRagas(RAG Assessment)が登場します。Ragasは、RAGパイプラインを評価するのに役立つフレームワークです。Ragasはオープンスタンダードの作成を目指しており、RAGアプリケーションで継続的な学習を活用するためのツールとテクニックを開発者に提供しています。詳細については、を参照してください https://docs.ragas.io/en/stable/getstarted/index.html["Ragasの使用を開始する"^]



== ラマ3

メタのLlama 3はデコーダ専用のトランスモデルであり、事前に学習されたLLM（Large Language Model）に公開されている。Llama 3は、15兆を超えるデータトークンでトレーニングされ、自然言語理解(NLU)におけるゲームチェンジャーです。コンテキストの理解や、翻訳やダイアログ生成などの複雑なタスクに優れています。llama 3には、効率的な導入と開発のための8Bと、大規模なAIネイティブアプリケーション向けの70Bの2種類のサイズがあります。Llama 3は、Vertex AIを介してGoogle Cloudに、Azure AI Studioを介してAzureに、Amazon Sagemakerを介してAWSに導入できます。

検証では、NVIDIA Nemo™マイクロサービスを搭載したMetaのLlamaモデルをNVIDIA A100 GPUでアクセラレーションされたNVIDIA DGXインスタンスに導入し、生成AIのユースケースをカスタマイズして評価し、オンプレミスのアプリケーションでの検索拡張生成(RAG)をサポートします。



== オープンソースフレームワーク

導入環境によっては、オープンソーステクノロジに関する次の追加情報が関係する場合があります。



=== ラングチェーン

LangChainは、大規模言語モデル(LLM)を基盤としたアプリケーションを開発するためのオープンソースの統合フレームワークです。Document Loader、VectorStores、およびその他のさまざまなパッケージが付属しているため、お客様はRAGアプリケーションを効率的に構築できます。これにより、開発者は複雑なワークフローを柔軟に構築できます。また、LangSmithを使用してアプリを検査、監視、評価し、LangServeを使用してREST APIにチェーンを常に最適化してデプロイすることもできます。LangChainはRAGアプリケーションのベストプラクティスをエンコードし、RAGアプリケーションの構築に必要なさまざまなコンポーネントの標準インターフェイスを提供します。



=== ラマインデックス

LlamaIndexは、カスタムデータソースをLLMベースのアプリケーションに接続するためのシンプルで柔軟なデータフレームワークです。柔軟なデータコネクタを介して、API、データベース、PDFなどからデータを取り込むことができます。Llama 3やGPT-4のようなLLMは、大規模なパブリックデータセットで事前にトレーニングされており、即座に運用できる驚異的な自然言語処理機能を備えています。ただし、それらのユーティリティは、あなた自身のプライベートデータにアクセスすることなく制限されています。LlamaIndexは、非常に人気のあるPythonおよびTypeScriptライブラリを提供し、検索拡張生成(RAG)技術で業界をリードしています。



== NVIDIA Nemoマイクロサービス

NVIDIA Nemoは、クラウドやデータセンターのどこにでも導入できる、エンタープライズクラスの生成AIモデルを構築およびカスタマイズするためのエンドツーエンドのプラットフォームです。Nemoは、生成型AIの開発と導入のプロセスを大規模に簡易化するマイクロサービスを提供しています。これにより、組織はLLMをエンタープライズデータソースに接続できます。本執筆時点では、NemoマイクロサービスはNVIDIAの早期アクセスプログラムを介して利用可能である。



=== NVIDIA Nemo推論マイクロサービス（NIMS）

NVIDIA AI Enterpriseの一部であるNVIDIA NIMSは、AIを活用したエンタープライズアプリケーションを開発し、AIモデルを本番環境に導入するための合理的な道筋を提供します。NIMSは、業界標準のAPI、ドメイン固有のコード、最適化された推論エンジン、エンタープライズランタイムを含む、コンテナ化された推論マイクロサービスです。



=== NVIDIA Nemo Retriever

NVIDIA Nemoフレームワークの最新サービスであるNVIDIA Nemo Retrieverは、RAGの埋め込みと取得の部分を最適化し、より高い精度とより効率的な応答を提供します。NVIDIA Nemo Retrieverは、オンプレミスまたはクラウドに導入できる情報検索サービスです。エンタープライズクラスのRAG機能をカスタマイズした本番用AIアプリケーションに統合するための、セキュアでシンプルな方法を提供します。



== NVIDIA Enterprise RAG LLMオペレータ

NVIDIA Enterprise Retrieval Augmented Generation（RAG）Large Language Model（LLM）Operatorは、KubernetesでRAGパイプラインを実行するために必要なソフトウェアコンポーネントとサービスを可能にします。NVIDIA Inference MicroserviceやNVIDIA Nemo Retriever Embedding Microserviceなど、RAGパイプラインの主要コンポーネントのライフサイクルを管理するオペレータへの早期アクセスを提供します。詳細については、を参照してください https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["NVIDIA Enterprise RAG LLMオペレータ"^]



== ベクターデータベース



=== PostgreSQL:pgvector

XGBoostなどの多くの古典的なMLアルゴリズムのネイティブバインディングにより、SQLで機械学習を行うことはPostgreSQLにとって新しいことではありません。最近では、ベクトル類似性検索のオープンソース拡張であるpgvectorのリリースにより、PostgreSQLはMLで生成された埋め込みを保存および検索する機能を備えています。これは、AIのユースケースやLLMを使用するアプリケーションに便利な機能です。

NVIDIA Enterprise RAG LLMオペレータを使用した検証では、デフォルトのサンプルパイプラインでpgvectorデータベースがポッド内で開始されます。その後、クエリサーバーはpgvectorデータベースに接続し、埋め込みを保存して取得します。チャットボットのWebアプリケーションとクエリサーバーは、マイクロサービスとベクターデータベースと通信して、ユーザーのプロンプトに応答します。



=== ミルバス

Milvusは、MongoDBと同様にAPIを提供する汎用性の高いベクターデータベースとして、多種多様なデータ型をサポートし、マルチベクトル化などの機能を備えているため、データサイエンスや機械学習に広く利用されています。ディープニューラルネットワーク（DNN）モデルと機械学習（ML）モデルによって生成された10億を超える埋め込みベクトルを格納、インデックス化、管理する能力を備えています。お客様は、Nvidia NIM & NemoマイクロサービスとMilvusをベクターデータベースとして使用してRAGアプリケーションを構築できます。NVIDIA Nemoコンテナが埋め込み生成のために正常にデプロイされたら、Milvusコンテナをデプロイしてそれらの埋め込みを保存できます。ベクターデータベースおよびNetAppの詳細については、を参照してください。 https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["リファレンスアーキテクチャ–Vector Database解決策with NetApp"^]。



=== Apache Cassandra

Apache Cassandra®は、オープンソースのNoSQLで、拡張性と可用性に優れたデータベースです。ベクトル検索機能を備えており、ベクトルデータ型とベクトル類似性検索機能をサポートしています。特に、LLMやプライベートRAGパイプラインを含むAIアプリケーションで役立ちます。

NetApp Instaclustrは、クラウドまたはオンプレミスでホストされるApache Cassandra®向けのフルマネージドサービスを提供します。これにより、NetAppのお客様は、Apache Cassandra®クラスタをプロビジョニングし、InstaclustrコンソールまたはInstaclstrプロビジョニングAPIを介してC#、Node.js、AWS PrivateLinkなどのさまざまなオプションを使用してクラスタに接続できます。

さらに、NetApp ONTAPは、Kubernetesで実行されるコンテナ化されたApache Cassandraクラスタの永続的ストレージプロバイダとして機能します。NetApp Astra Controlは、ONTAPのデータ管理のメリットを、Apache Cassandraなどの大量のデータを扱うKubernetesアプリケーションにシームレスに拡張します。詳細については、を参照してください。 https://cloud.netapp.com/hubfs/SB-4134-0321-DataStax-Cassandra-Guide%20(1).pdf["NetApp Astra ControlとONTAPストレージを使用したDataStax Enterprise向けのアプリケーション対応データ管理"^]



=== NetApp Instaclustr

Instaclustrは、オープンソーステクノロジ向けSaaSプラットフォームを通じてデータインフラをサポートすることで、組織が大規模なアプリケーションを提供できるよう支援します。検索アプリケーションに意味理解を組み込みたいと考えているジェネレーティブAI開発者には、さまざまなオプションがあります。Instaclustr for Postgresはpgvector拡張をサポートしています。Instaclustr for OpenSearchは、ベクター検索をサポートしており、入力クエリに基づいて関連ドキュメントを検索し、最も近いネイバー関数を使用します。Instaclustr for Redisは、ベクターデータの保存、ベクターの取得、およびベクター検索を実行できます。詳細については、 https://www.instaclustr.com/platform/["NetAppによるInstaclustrプラットフォーム"^]



== NetApp BlueXP

NetApp BlueXPは、ネットアップのすべてのストレージサービスとデータサービスを1つのツールに統合し、ハイブリッドマルチクラウドのデータ資産を構築、保護、管理できます。オンプレミス環境とクラウド環境にわたってストレージとデータサービスのエクスペリエンスを統一し、柔軟な消費パラメータと、今日のクラウド主導の世界に必要な統合された保護機能を備えたAIOpsによる運用の簡易化を実現します。



== NetApp Cloud Insights の略

NetApp Cloud Insights は、インフラ全体を可視化できるクラウドインフラ監視ツールです。Cloud Insights を使用すると、パブリッククラウドやプライベートデータセンターなど、すべてのリソースの監視、トラブルシューティング、最適化を行うことができます。Cloud Insightsは、Kubernetesを含む異機種混在のインフラやワークロード向けに、数百のコレクタからインフラとアプリケーションを完全に可視化します。詳細については、を参照してください https://docs.netapp.com/us-en/cloudinsights/index.html["Cloud Insights でできること"^]



== NetApp StorageGRID

NetApp StorageGRID は、ソフトウェアで定義されるオブジェクトストレージスイートで、パブリック、プライベート、ハイブリッドのマルチクラウド環境での幅広いユースケースに対応します。StorageGRID はAmazon S3 APIをネイティブでサポートし、自動化されたライフサイクル管理などの業界をリードする革新的なテクノロジを提供して、非構造化データを長期にわたってコスト効率よく格納、保護、保持します。



== NetAppスポット

Spot by NetAppは、AWS、Azure、Google Cloudのクラウドインフラを自動化、最適化し、SLAで保証された可用性とパフォーマンスを最小限のコストで実現します。Spotは、機械学習と分析アルゴリズムを使用して、スポット容量を本番環境やミッションクリティカルなワークロードに活用できるようにします。GPUベースのインスタンスを実行しているお客様は、Spotのメリットを活用し、コンピューティングコストを削減できます。



== NetApp ONTAP

ネットアップが提供する最新世代のストレージ管理ソフトウェアONTAP 9を使用すれば、インフラを最新化し、クラウド対応のデータセンターに移行できます。ONTAP は、業界をリードするデータ管理機能を活用して、データの格納場所に関係なく、単一のツールセットでデータの管理と保護を実現します。エッジ、コア、クラウドなど、必要な場所に自由にデータを移動することもできます。ONTAP 9には、データ管理の簡易化、重要なデータの高速化と保護、ハイブリッドクラウドアーキテクチャ全体で次世代インフラ機能を実現する多数の機能が搭載されています。



=== データ管理を簡易化

データ管理は、AIアプリケーションの運用やAI / MLデータセットのトレーニングに適切なリソースを使用できるように、エンタープライズIT運用とデータサイエンティストにとって非常に重要です。以下に記載するネットアップテクノロジに関する追加情報 は、この検証の対象外ですが、導入環境によっては関連性がある場合もあります。

ONTAP データ管理ソフトウェアには、運用を合理化および簡易化し、総運用コストを削減するための次の機能が含まれています。

* インラインデータコンパクション、強化された重複排除：データコンパクションはストレージブロック内の無駄なスペースを削減し、重複排除は実効容量を大幅に増やします。この環境データはローカルに格納され、データはクラウドに階層化されます。
* 最小、最大、アダプティブのQuality of Service（AQoS）。きめ細かいサービス品質（QoS）管理機能により、高度に共有された環境で重要なアプリケーションのパフォーマンスレベルを維持できます。
* NetApp FabricPool の略。Amazon Web Services（AWS）、Azure、NetApp StorageGRID ストレージ解決策 など、パブリッククラウドとプライベートクラウドのストレージオプションへコールドデータを自動的に階層化します。FabricPool の詳細については、を参照してください https://www.netapp.com/pdf.html?item=/media/17239-tr4598pdf.pdf["TR-4598：『FabricPool best bests』"^]。




=== データの高速化と保護

ONTAP は、卓越したパフォーマンスとデータ保護を実現し、以下の方法でこれらの機能を拡張します。

* パフォーマンスとレイテンシの低下：ONTAP は、可能なかぎり最小のレイテンシで最高のスループットを提供します。
* データ保護ONTAP には、組み込みのデータ保護機能が用意されており、すべてのプラットフォームを共通の管理機能で管理できます。
* NetApp Volume Encryption（NVE）：ONTAP は、オンボードと外部キー管理の両方をサポートし、ボリュームレベルでのネイティブな暗号化を実現します。
* マルチテナンシーおよび多要素認証ONTAP を使用すると、最高レベルのセキュリティでインフラリソースを共有できます。




=== 将来のニーズにも対応できるインフラ

ONTAP は、次の機能を備えており、要件が厳しく、絶えず変化するビジネスニーズに対応できます。

* シームレスな拡張とノンストップオペレーションONTAP を使用すると、既存のコントローラとスケールアウトクラスタに無停止で容量を追加できます。NVMe や 32Gb FC などの最新テクノロジへのアップグレードも、コストのかかるデータ移行やシステム停止を行わずに実行できます。
* クラウドへの接続：ONTAPは、ほとんどのクラウドに対応したストレージ管理ソフトウェアで、すべてのパブリッククラウドでSoftware-Defined Storageとクラウドネイティブインスタンスを選択できます。
* 新しいアプリケーションとの統合：ONTAP は、既存のエンタープライズアプリケーションをサポートするインフラを使用して、自律走行車、スマートシティ、インダストリー4.0などの次世代プラットフォームやアプリケーション向けにエンタープライズクラスのデータサービスを提供します。




== NetApp ONTAP 対応の Amazon FSX

Amazon FSx for NetApp ONTAPは、ネットアップが人気のONTAPファイルシステムを基盤に構築された、信頼性、拡張性、パフォーマンス、機能豊富なファイルストレージを提供する、ファーストパーティのフルマネージドAWSサービスです。FSX for ONTAP は、ネットアップファイルシステムの使い慣れた機能、パフォーマンス、機能、API操作に、AWSのフルマネージドサービスならではの即応性、拡張性、シンプルさを兼ね備えています。



== Azure NetApp Files の特長

Azure NetApp Filesは、Azureネイティブのファーストパーティ機能を備えたエンタープライズクラスのハイパフォーマンスファイルストレージサービスです。SMB、NFS、デュアルプロトコルのボリュームをサポートしており、次のようなユースケースに使用できます。

* ファイル共有：
* ホームディレクトリ：
* データベース：
* ハイパフォーマンスコンピューティング：
* ジェネレーティブAI：




== Google Cloud NetAppボリューム

Google Cloud NetApp Volumesは、高度なデータ管理機能と拡張性に優れたパフォーマンスを提供する、フルマネージドのクラウドベースデータストレージサービスです。ネットアップがホストするデータは、プレビュー版ツールキットのリファレンスアーキテクチャで、GoogleのVertex AIプラットフォームのRAG（検索拡張生成）処理で使用できます。



== ネットアップアストラト Trident

Astra Tridentでは、パブリッククラウドやオンプレミスにあるONTAP（AFF、NetApp FAS、Select、Cloud、 Amazon FSx for NetApp ONTAP）、Elementソフトウェア（NetApp HCI、SolidFire）、Azure NetApp Filesサービス、Cloud Volumes Service on Google CloudAstra Tridentは、Kubernetesとネイティブに統合される、コンテナストレージインターフェイス（CSI）に準拠した動的ストレージオーケストレーションツールです。



== Kubernetes

Kubernetes は、 Google が当初設計した、オープンソースの分散型コンテナオーケストレーションプラットフォームであり、 Cloud Native Computing Foundation （ CNCF ）によって管理されています。Kubernetesは、コンテナ化されたアプリケーションの導入、管理、拡張の自動化機能を可能にし、エンタープライズ環境における主要なコンテナオーケストレーションプラットフォームです。
