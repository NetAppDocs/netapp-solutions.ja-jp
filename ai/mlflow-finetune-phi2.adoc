---
sidebar: sidebar 
permalink: ai/aicp_example_notebooks_and_pipelines.html 
keywords: Jupyter Notebook, MLFlow, NetApp DataOps Toolkit, LLM, 
summary: Jupyter HubでMLFlowを使用した大規模言語モデルの微調整 
---
= 前提条件
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
ここでは、Jupyter Hubを使用してMLFlowでLarge Language Model (LLM)を微調整する手順について説明します。これは、取得拡張生成（RAG）などのお客様のユースケース向けに、NetAppストレージとNetAppインテリジェントデータインフラを組み込んだトレーニングジョブの例を示すことを目的としています。



== 前提条件

このセクションでは、jupyter hubを使用して言語モデルを微調整するための前提条件について説明します。そのためには、モデルのトレーニングや微調整に必要な関連ライブラリとパッケージがすでにインストールされていることを前提としています。この例で使用されているライブラリには、以下のものが含まれますが、これらに限定されません。-transformers-peft(Parameter Efficient Fine Tuning)- accelerateこれらはHuggingFaceに属するライブラリです。その他のライブラリには、matplotlib、SciPy、Einopsなどがある。

また、HuggingFaceを介してベースモデルとそのウェイトにアクセスできることも前提としています。使用可能なモデルのリストは、で確認 https://huggingface.co/models["ハギングフェイス"]できます。

最後に、適切なストレージを持つJupyter Hubアカウントにアクセスする必要があります。GPUサーバにアクセスできることを推奨します（コンピューティング要件が高い場合）。

この微調整の例は、によって開発されたノートブックガイドと例のコレクションからヒントを得て https://github.com/brevdev/notebooks["Brevdevチーム"]います。



== データのロードと測定条件の設定

すべてのデータ(ドキュメントとテキスト)をノートブックと同じ共有フォルダに保存して、簡単に取得できます。Data Processingおよびトレーニング用にドキュメントを.json形式に変換します。

データが処理されたら、データとともにモデルをロードするのに十分なRAMがGPUにあることを確認します。この例では、16 GBのRAMを搭載したNVIDIA Tesla T4 GPUを使用して、Microsoftの https://huggingface.co/microsoft/phi-2["PHI-2"]27億のパラメータの微調整を行います。使用するモデルやデータ量によっては、分散型のトレーニングセットアップを構築する場合に備えて、より多くのRAMやGPUを追加する必要がある場合があります。

image:tesla-t4.png["この例で使用しているGPUの仕様"]

この例では、 https://huggingface.co/docs/diffusers/main/en/training/lora["ロラ"]を使用して、モデルがリソース制約に準拠していることを確認します。計算が制約でない場合は、完全な微調整を実行できます。



== 成果と今後の作業

次の表は、NetAppソリューションデータセットでトレーニングされたPhi-2のトレーニング損失とそれに対応する検証損失を示しています(内部ソリューションエンジニアリングブログ/記事)。観察されたように、検証損失は、エポックの数が増加するにつれてトレーニング損失と一緒に減少します。

image:phi2-results.png["Phi-2のトレーニング損失と検証損失"]

微調整された言語モデルを評価するために使用されるいくつかのサンプルプロンプトは、以下のとおりです。AIトレーニングデータセットにNetApp FlexGroupボリュームを使用するメリットを説明する*

このプロンプトに対する応答は次のようになります。「NetApp FlexGroupボリュームをAIトレーニングデータセットに使用すると、次のようなメリットがあります。-*拡張性*：AIトレーニングに必要な大量のデータとコンピューティングリソースに対応するために、FlexGroupボリュームを水平方向に拡張できます。これにより、リソースの制限を気にすることなく、大規模なデータセットでモデルをトレーニングできます。-*データの整合性*：FlexGroupボリュームは、すべてのデータに単一のネームスペースを提供することで、クラスタ内の複数のノード間でデータの整合性を確保します。これにより、異なるノード間でデータの可用性と一貫性が常に確保されます。これは、複数のソースのデータにアクセスする必要があるAIアプリケーションにとって重要です。-*パフォーマンス*：FlexGroupボリュームはハイパフォーマンスストレージ向けに最適化されているため、AIトレーニングに必要な大規模データセットの格納に最適です。ボリュームのパフォーマンス特性には、低レイテンシ、高スループット、高可用性などがあり、トレーニング中にデータへの迅速かつ確実なアクセスが確保されます。-*セキュリティ*：FlexGroupボリュームは、暗号化、アクセス制御、データ保護などの組み込みのセキュリティ機能を提供し、ライフサイクル全体にわたって機密データを確実に保護します。これは、個人情報や財務データなどの機密データを扱うAIアプリケーションで特に重要です。全体的に、NetApp FlexGroupボリュームは、AIトレーニングデータセットを格納、管理するための、拡張性に優れたセキュアでハイパフォーマンスなソリューションを提供します」

*2.ONTAP 9はどのような利点と機能を提供しますか。具体的な事例を挙げてください。*

このプロンプトに対する応答は次のようになりますONTAP 9。-*ハイパフォーマンス*、ノードとストレージボリューム間のデータ転送用にノードあたり最大1TB/秒のスループットを実現します。-*複数のノードにわたって99.999%のアップタイムと高可用性を実現する高可用性*。-*拡張性*。ノードを追加したり、各ノードのディスク数を増やしたりして、横方向に拡張できます。-*柔軟性*。ワークロードの要件に応じて、SSD、HDD、NVMeドライブなど、さまざまなタイプのストレージボリュームを使用できます。-*セキュリティ*。暗号化、アクセス制御、監査などのさまざまなセキュリティポリシーを実装して、クラスタ内のデータとリソースを保護します。-データ重複排除、圧縮、レプリケーション*などの高度な機能をサポートし、パフォーマンスを最適化し、大量データの保存および転送コストを削減します。

これらの応答は、微調整されたモデルが、ベースモデルと比較してトレーニングされたデータセットからより豊かなコンテキストを持ち、より関連性の高い出力を引き出すことを示しています。

この例は、少量のコンピュータリソースを持つカスタム/プロプライエタリデータで言語モデルをトレーニングすることがまだ可能であることを示すことを目的としています。将来的には、より大規模なGPUセットアップ（GPUの分散システムネットワーク）を活用して、より大きな言語モデル（10B以上のパラメータの順序）を組織全体のデータでトレーニングすることも予定されています。

link:mlrun_configure_working_environment.adoc["次の記事：ML実行作業環境"]
