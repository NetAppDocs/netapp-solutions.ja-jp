---
sidebar: sidebar 
permalink: ai/ai-edge-test-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: このセクションでは、この解決策の検証に使用するテスト手順について説明します。 
---
= 手順をテストします
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
このセクションでは、この解決策の検証に使用するテスト手順について説明します。



== オペレーティングシステムと AI 推論のセットアップ

AFF C190では、Ubuntu 18.04とNVIDIAドライバ、NVIDIA GPUをサポートするDockerを使用し、LenovoがMLPerf Inference v0.7に提出する際に利用可能なMLPerfを使用し https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["コード"^]ました。

EF280では、Ubuntu 20.04とNVIDIAドライバを使用し、NVIDIA GPUとMLPerfをサポートしたDockerを使用して、LenovoからMLPerf https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["コード"^] Inference v1.1に提出しました。

AI 推論をセットアップするには、次の手順を実行します。

. 登録が必要なデータセット、 ImageNet 2012 Validation set 、 Crito Terabyte データセット、および BRT 2019 Training セットをダウンロードし、ファイルを解凍します。
. 1TB以上の作業ディレクトリを作成し、そのディレクトリを参照する環境変数を定義します `MLPERF_SCRATCH_PATH`。
+
このディレクトリは、ネットワークストレージのユースケース用に共有ストレージ上で共有するか、またはローカルデータでテストする際にローカルディスク上で共有する必要があります。

. makeコマンドを実行し `prebuild`ます。このコマンドは、必要な推論タスク用にDockerコンテナを構築して起動します。
+

NOTE: 実行中の Docker コンテナ内から次のコマンドがすべて実行されます。

+
** MLPerf推論タスク向けのトレーニング済みAIモデルをダウンロード： `make download_model`
** 無料でダウンロード可能な追加データセットのダウンロード： `make download_data`
** データの前処理：make `preprocess_data`
** を実行します `make build`。
** コンピューティングサーバのGPU向けに最適化された推論エンジンを構築します。 `make generate_engines`
** 推論ワークロードを実行するには、次のコマンドを実行します（ 1 つのコマンド）。




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== AI 推論の実行

実行された実行のタイプは次の 3 つです。

* ローカルストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用したマルチサーバ AI 推論

