---
sidebar: sidebar 
permalink: ai/ai-edge-test-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: このセクションでは、この解決策の検証に使用するテスト手順について説明します。 
---
= 手順をテストします
:hardbreaks:
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:ai-edge-test-configuration.html["前の手順：設定をテストします。"]

この検証では次のテスト手順を使用しました。



=== オペレーティングシステムと AI 推論のセットアップ

AFF C190 には、 NVIDIA ドライバと Docker を搭載した Ubuntu 18.04 を使用し、 NVIDIA GPU をサポートし、 MLPerf を使用しました https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["コード"^] Lenovo から MLPerf Inference v0.7 への提出書類の一部として提供されます。

EF280 には、 NVIDIA ドライバと Docker を搭載した Ubuntu 20.04 を使用し、 NVIDIA GPU と MLPerf をサポートしました https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["コード"^] Lenovo から MLPerf Inference v1.1 への提出の一部として提供されています。

AI 推論をセットアップするには、次の手順を実行します。

. 登録が必要なデータセットをダウンロードします。 ImageNet 2012 Validation set 、 Crito Terabyte dataset 、 BRT 2019 Training set の順に選択し、ファイルを解凍します。
. 1TB 以上の作業ディレクトリを作成し、ディレクトリを参照する環境変数 MLPERF_scratch_path を定義します。
+
このディレクトリは、ネットワークストレージのユースケース用に共有ストレージ上で共有するか、またはローカルデータでテストする際にローカルディスク上で共有する必要があります。

. make prebuild コマンドを実行します。このコマンドは、必要な推論タスク用の Docker コンテナを構築して起動します。
+

NOTE: 実行中の Docker コンテナ内から次のコマンドがすべて実行されます。

+
** MLPerf Inference タスク用のトレーニング済み AI モデルをダウンロード： DOWNLOAD_MODEL
** 無償でダウンロードできる追加のデータセットをダウンロードします。 make download_data
** データの前処理： make preprocess_data
** 実行： make build
** コンピューティングサーバの GPU に最適化された推論エンジンを構築します。 make generate_engines
** 推論ワークロードを実行するには、次のコマンドを実行します（ 1 つのコマンド）。




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


=== AI 推論の実行

実行された実行のタイプは次の 3 つです。

* ローカルストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用したマルチサーバ AI 推論


link:ai-edge-test-results.html["次の手順：テスト結果"]
