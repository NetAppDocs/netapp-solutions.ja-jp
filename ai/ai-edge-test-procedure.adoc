---
sidebar: sidebar 
permalink: ai/ai-edge-test-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: このセクションでは、この解決策の検証に使用するテスト手順について説明します。 
---
= 手順をテストします
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
このセクションでは、この解決策の検証に使用するテスト手順について説明します。



== オペレーティングシステムと AI 推論のセットアップ

AFF C190 には、 NVIDIA ドライバと Docker を搭載した Ubuntu 18.04 を使用し、 NVIDIA GPU をサポートし、 MLPerf を使用しました https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["コード"^] Lenovo から MLPerf Inference v0.7 への提出書類の一部として提供されます。

EF280 には、 NVIDIA ドライバと Docker を搭載した Ubuntu 20.04 を使用し、 NVIDIA GPU と MLPerf をサポートしました https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["コード"^] Lenovo から MLPerf Inference v1.1 への提出の一部として提供されています。

AI 推論をセットアップするには、次の手順を実行します。

. 登録が必要なデータセット、 ImageNet 2012 Validation set 、 Crito Terabyte データセット、および BRT 2019 Training セットをダウンロードし、ファイルを解凍します。
. 1TB 以上の作業ディレクトリを作成し、ディレクトリを参照する環境変数「 M LPERF_scratch_path 」を定義します。
+
このディレクトリは、ネットワークストレージのユースケース用に共有ストレージ上で共有するか、またはローカルデータでテストする際にローカルディスク上で共有する必要があります。

. make 「 prebuild 」コマンドを実行します。このコマンドは、必要な推論タスク用の Docker コンテナを構築して起動します。
+

NOTE: 実行中の Docker コンテナ内から次のコマンドがすべて実行されます。

+
** MLPerf Inference タスク用のトレーニング済み AI モデル「 make download_model 」をダウンロードしてください
** 無料でダウンロードできる追加のデータセット「 make download_data 」をダウンロードしてください
** データをプリプロセスします。「 preprocess_data 」にします
** 「 make build 」を実行します。
** コンピューティングサーバの GPU に最適化された推論エンジン「 generate_engines 」を構築します
** 推論ワークロードを実行するには、次のコマンドを実行します（ 1 つのコマンド）。




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== AI 推論の実行

実行された実行のタイプは次の 3 つです。

* ローカルストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用した単一サーバの AI 推論
* ネットワークストレージを使用したマルチサーバ AI 推論

