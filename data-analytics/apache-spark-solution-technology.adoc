---
sidebar: sidebar 
permalink: data-analytics/apache-spark-solution-technology.html 
keywords: standalone, apache mesos, hadoop yarn, resilient distributed dataset, rdd, dataframe, hadoop distributed file system, hdfs 
summary: このセクションでは、Apache Sparkの性質とコンポーネント、およびそれらがこの解決策 にどのように貢献するかについて説明します。 
---
= 解決策テクノロジ
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Apache Sparkは、Hadoop Distributed File System（HDFS）と直接連携するHadoopアプリケーションを作成するための一般的なプログラミングフレームワークです。Sparkは本番環境に対応しており、ストリーミングデータの処理をサポートしています。MapReduceよりも高速です。Sparkには、効率的なイテレーションのためのメモリ内データキャッシングが設定可能であり、Sparkシェルはデータの学習と探索のためにインタラクティブです。Sparkでは、Python、Scala、Javaでアプリケーションを作成できます。Sparkアプリケーションは、1つ以上のタスクを持つ1つ以上のジョブで構成されています。

すべてのSparkアプリケーションにはSparkドライバがあります。yarn -クライアントモードでは、ドライバはクライアント上でローカルに実行されます。Yarn -クラスタモードでは、ドライバはアプリケーションマスター上のクラスタで実行されます。クラスタモードでは、クライアントが切断してもアプリケーションは引き続き実行されます。

image:apache-spark-image3.png["エラー：グラフィックイメージがありません"]

クラスタマネージャは3種類あります。

* *スタンドアロン。*このマネージャーはSparkの一部であり、クラスタのセットアップが簡単です。
* * Apache Mesos.* MapReduceなどのアプリケーションも実行される一般的なクラスタマネージャです。
* * Hadoop糸。*これはHadoop 3のリソースマネージャーです。


レジリエントな分散データセット（RDD）はSparkの主要コンポーネントです。RDDは、クラスタ内のメモリに格納されているデータから、失われたデータや欠落しているデータを再作成し、ファイルから取得した初期データまたはプログラムによって作成された初期データを格納します。RDDは、ファイル、メモリ内のデータ、または別のRDDから作成されます。Sparkプログラミングは、変換とアクションという2つの操作を実行します。既存のRDDに基づいて新しいRDDが作成されます。アクションはRDDから値を返します。

変換とアクションはSparkのデータセットとDataFramesにも適用されます。データセットとは、分散されたデータの集合であり、Spark SQLの最適化された実行エンジンの利点とともに、RDDの利点（強力なタイピング、ラムダ関数の使用）を提供します。データセットはJVMオブジェクトから作成し、機能変換（マップ、フラットマップ、フィルタなど）を使用して操作できます。DataFrameは、名前付き列に編成されたデータセットです。概念的には、リレーショナルデータベースのテーブルまたはR/Pythonのデータフレームに相当します。データフレームは、構造化データファイル、HiveまたはHBaseのテーブル、オンプレミスまたはクラウドの外部データベース、既存のRDDなど、さまざまなソースから構築できます。

Sparkアプリケーションには1つ以上のSparkジョブが含まれています。ジョブは実行者でタスクを実行し、実行者はYarnコンテナで実行します。各実行者は1つのコンテナで実行され、実行者はアプリケーションのライフサイクルを通して存在します。実行者はアプリケーションの起動後に修正され、yarnは割り当て済みのコンテナのサイズを変更しません。実行者は、メモリ内のデータに対してタスクを同時に実行できます。
